{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f040145",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                      | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HkeaLA39Ym rJl0r3R9KX BJpIR25YX\n",
      "HkeaLA39Ym rJl0r3R9KX Sk_lKX9Tm\n",
      "HkeaLA39Ym rJl0r3R9KX ry79jm5pQ\n",
      "HkeaLA39Ym rJl0r3R9KX ByCx70sam\n",
      "HkeaLA39Ym rJl0r3R9KX BJVDsE367\n",
      "HkeaLA39Ym rJl0r3R9KX rkXUYERSN\n",
      "HkeaLA39Ym rJl0r3R9KX BkaTYNRS4\n",
      "HkeaLA39Ym rJl0r3R9KX rtZtjY_fk9\n",
      "{'cdate': 1541203171721,\n",
      " 'content': {'confidence': '3: The reviewer is fairly confident that the '\n",
      "                           'evaluation is correct',\n",
      "             'rating': '7: Good paper, accept',\n",
      "             'review': 'Authors proposes a new algorithm for improving the '\n",
      "                       'stability of class importance weighting estimation '\n",
      "                       'procedure (Lipton et al., 2018) with a two-step '\n",
      "                       'procedure. The reparamaterization of w using the '\n",
      "                       'weight shift theta and lambda allows authors develop a '\n",
      "                       'generalization upperbound with terms rely on theta, '\n",
      "                       'sigma and lambda. \\n'\n",
      "                       '\\n'\n",
      "                       'The problem of label shift is a known important issue '\n",
      "                       'in transfer learning but has been understudied.\\n'\n",
      "                       '\\n'\n",
      "                       'The paper is very well written and the algorithm is '\n",
      "                       'well-motivated (introducing regularization to avoid '\n",
      "                       'the singularity) and post processing step looks sound '\n",
      "                       '(using lambda to de-biase). I only have a few minor '\n",
      "                       'questions: \\n'\n",
      "                       '\\n'\n",
      "                       '1. How realistic it is to assume we have prior '\n",
      "                       'knowledge on theta and sigma_min? \\n'\n",
      "                       '\\n'\n",
      "                       '2. If I understand correctly, the only experiment '\n",
      "                       'where lambda is varied is Sec 3.3? It would be '\n",
      "                       'interesting if authors also included BBSE in Sec 3.3 '\n",
      "                       'as a baseline. \\n'\n",
      "                       '\\n'\n",
      "                       '3. The authors mentioned in the discussion that the '\n",
      "                       'generalization guarantee is obtained with no prior '\n",
      "                       \"knowledge q/p is needed. However, doesn't theta \"\n",
      "                       'implicitly represent the knowledge in p/q?   ',\n",
      "             'title': 'Interesting Algorithm with Solid Theories'},\n",
      " 'ddate': None,\n",
      " 'details': None,\n",
      " 'forum': 'rJl0r3R9KX',\n",
      " 'id': 'r1h1rvc3Q',\n",
      " 'invitation': 'ICLR.cc/2019/Conference/-/Paper1592/Official_Review',\n",
      " 'mdate': None,\n",
      " 'nonreaders': [],\n",
      " 'number': 3,\n",
      " 'original': None,\n",
      " 'readers': ['everyone'],\n",
      " 'referent': 'r1ghyrDc2m',\n",
      " 'replyto': 'rJl0r3R9KX',\n",
      " 'signatures': ['ICLR.cc/2019/Conference/Paper1592/AnonReviewer3'],\n",
      " 'tcdate': 1541203171721,\n",
      " 'tmdate': 1541533006745,\n",
      " 'writers': ['ICLR.cc/2019/Conference']}\n",
      "{'cdate': 1543160898185,\n",
      " 'content': {'confidence': '3: The reviewer is fairly confident that the '\n",
      "                           'evaluation is correct',\n",
      "             'rating': '7: Good paper, accept',\n",
      "             'review': 'Authors proposes a new algorithm for improving the '\n",
      "                       'stability of class importance weighting estimation '\n",
      "                       'procedure (Lipton et al., 2018) with a two-step '\n",
      "                       'procedure. The reparamaterization of w using the '\n",
      "                       'weight shift theta and lambda allows authors develop a '\n",
      "                       'generalization upperbound with terms rely on theta, '\n",
      "                       'sigma and lambda. \\n'\n",
      "                       '\\n'\n",
      "                       'The problem of label shift is a known important issue '\n",
      "                       'in transfer learning but has been understudied.\\n'\n",
      "                       '\\n'\n",
      "                       'The paper is very well written and the algorithm is '\n",
      "                       'well-motivated (introducing regularization to avoid '\n",
      "                       'the singularity) and post processing step looks sound '\n",
      "                       '(using lambda to de-biase). I only have a few minor '\n",
      "                       'questions: \\n'\n",
      "                       '\\n'\n",
      "                       '1. How realistic it is to assume we have prior '\n",
      "                       'knowledge on theta and sigma_min? \\n'\n",
      "                       '\\n'\n",
      "                       '2. If I understand correctly, the only experiment '\n",
      "                       'where lambda is varied is Sec 3.3? It would be '\n",
      "                       'interesting if authors also included BBSE in Sec 3.3 '\n",
      "                       'as a baseline. \\n'\n",
      "                       '\\n'\n",
      "                       '3. The authors mentioned in the discussion that the '\n",
      "                       'generalization guarantee is obtained with no prior '\n",
      "                       \"knowledge q/p is needed. However, doesn't theta \"\n",
      "                       'implicitly represent the knowledge in p/q?   \\n'\n",
      "                       '\\n'\n",
      "                       '------------------------------------------------\\n'\n",
      "                       '\\n'\n",
      "                       \"I have read authors' comments. \",\n",
      "             'title': 'Interesting Algorithm with Solid Theories'},\n",
      " 'ddate': None,\n",
      " 'details': None,\n",
      " 'forum': 'rJl0r3R9KX',\n",
      " 'id': 'ByqS4Sd0Q',\n",
      " 'invitation': 'ICLR.cc/2019/Conference/-/Paper1592/Official_Review/AnonReviewer3/Revision',\n",
      " 'mdate': None,\n",
      " 'nonreaders': [],\n",
      " 'number': 1,\n",
      " 'original': None,\n",
      " 'readers': ['everyone'],\n",
      " 'referent': 'r1ghyrDc2m',\n",
      " 'replyto': 'rJl0r3R9KX',\n",
      " 'signatures': ['ICLR.cc/2019/Conference/Paper1592/AnonReviewer3'],\n",
      " 'tcdate': 1543160898185,\n",
      " 'tmdate': 1543160898185,\n",
      " 'writers': ['ICLR.cc/2019/Conference/Paper1592/AnonReviewer3',\n",
      "             'ICLR.cc/2019/Conference']}\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████▌                                         | 1/10 [00:06<01:00,  6.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyl5Mc3qtX SylCrnCcFX Hk9Mq3ctm\n",
      "Hyl5Mc3qtX SylCrnCcFX Bk70YO_6X\n",
      "Hyl5Mc3qtX SylCrnCcFX SJyIUFu67\n",
      "Hyl5Mc3qtX SylCrnCcFX Hysw9K_pX\n",
      "Hyl5Mc3qtX SylCrnCcFX BkokwMAHN\n",
      "Hyl5Mc3qtX SylCrnCcFX H1IEk7RBN\n",
      "Hyl5Mc3qtX SylCrnCcFX HyMkSERBE\n",
      "Hyl5Mc3qtX SylCrnCcFX r1V_Zj184\n",
      "Hyl5Mc3qtX SylCrnCcFX SJd008eUE\n",
      "Hyl5Mc3qtX SylCrnCcFX rZ4W1cOzJ5\n",
      "{'cdate': 1541095999208,\n",
      " 'content': {'confidence': '3: The reviewer is fairly confident that the '\n",
      "                           'evaluation is correct',\n",
      "             'rating': '6: Marginally above acceptance threshold',\n",
      "             'review': 'A key challenge that presents the deep learning '\n",
      "                       'community is that state-of-the-art solutions are '\n",
      "                       'oftentimes associated with unstable derivatives, '\n",
      "                       'compromising the robustness of the network. In this '\n",
      "                       'paper, the author(s) explore the problem of how to '\n",
      "                       'train a neural network with stable derivatives by '\n",
      "                       'expanding the linear region associated with training '\n",
      "                       'samples. \\n'\n",
      "                       '\\n'\n",
      "                       'The author(s) studied deep networks with piecewise '\n",
      "                       'linear activations, which allow them to derive lower '\n",
      "                       'bounds on the $l_p$ margin with provably stable '\n",
      "                       'derivatives. In the special case of $l_2$ metric, this '\n",
      "                       'bound is analytic, albeit rigid and non-smooth. To '\n",
      "                       'avoid associated computational issues, the author(s) '\n",
      "                       'borrowed an idea from transductive/semi-supervised SVM '\n",
      "                       '(TSVM) to derive a relaxed formulation. \\n'\n",
      "                       '\\n'\n",
      "                       'In general, I find this paper rather interesting and '\n",
      "                       'well written. However, I do have a few concerns and '\n",
      "                       'confusions as listed below: \\n'\n",
      "                       '\\n'\n",
      "                       'Major ones:\\n'\n",
      "                       '\\n'\n",
      "                       '- I would prefer some elaborations on why the '\n",
      "                       'relaxation proposed in Eqn (8) serves to encourage the '\n",
      "                       \"margin of L2 ball? What's the working mechanism or \"\n",
      "                       'heuristic behind this relaxation? This is supposedly '\n",
      "                       'one of the key techniques used in optimization, yet '\n",
      "                       'remains obscure. \\n'\n",
      "                       '\\n'\n",
      "                       '- On empirical gains, the author(s) claimed that '\n",
      "                       '\"about 30 times larger margins can be achieved by '\n",
      "                       'trading off 1% accuracy.\"  It seems that consistently '\n",
      "                       'yields inferior prediction accuracy. In my opinion, '\n",
      "                       'the author(s) failed to showcase the practical utility '\n",
      "                       'of their solution. A better job should be done to '\n",
      "                       'validate the claim `` The problem we tackle has '\n",
      "                       'implications to interpretability and transparency of '\n",
      "                       \"complex models. '' \\n\"\n",
      "                       '\\n'\n",
      "                       '- As always, gradient-based penalties suffer from '\n",
      "                       'heavy computational overhead. The final objectives '\n",
      "                       'derived in this paper (Eqn (7) & Eqn (9)) seem no '\n",
      "                       'exception to this, and perhaps even worse since the '\n",
      "                       'gradient is taken wrt each neuron. Could the author(s) '\n",
      "                       'provide statistics on empirical wallclock performance? '\n",
      "                       'How much drain does this extra gradient penalty impose '\n",
      "                       'on the training efficiency? \\n'\n",
      "                       '\\n'\n",
      "                       'Minor ones:\\n'\n",
      "                       '\\n'\n",
      "                       '- Just to clarify, does the | - | used in Eqn (9) for '\n",
      "                       '|I(x,\\\\gamma)|  denote counting measure? \\n'\n",
      "                       '\\n'\n",
      "                       '- I do not see the necessity of introducing Lemma 7 in '\n",
      "                       'the text. Please explain. \\n'\n",
      "                       '\\n'\n",
      "                       '- Lemma 8, ``... then any optimal solutions for the '\n",
      "                       \"problem is also optimal for Eqn (4). '' Do you mean \"\n",
      "                       \"``the following problem'' (Eqn (5))? \\n\",\n",
      "             'title': 'It seems to be an interesting problem but is the '\n",
      "                      'solution proposed practical?'},\n",
      " 'ddate': None,\n",
      " 'details': None,\n",
      " 'forum': 'SylCrnCcFX',\n",
      " 'id': 'SJvrGad3Q',\n",
      " 'invitation': 'ICLR.cc/2019/Conference/-/Paper1591/Official_Review',\n",
      " 'mdate': None,\n",
      " 'nonreaders': [],\n",
      " 'number': 2,\n",
      " 'original': None,\n",
      " 'readers': ['everyone'],\n",
      " 'referent': 'H1eDBfaunm',\n",
      " 'replyto': 'SylCrnCcFX',\n",
      " 'signatures': ['ICLR.cc/2019/Conference/Paper1591/AnonReviewer2'],\n",
      " 'tcdate': 1541095999208,\n",
      " 'tmdate': 1541533007425,\n",
      " 'writers': ['ICLR.cc/2019/Conference']}\n",
      "{'cdate': 1542645739449,\n",
      " 'content': {'confidence': '3: The reviewer is fairly confident that the '\n",
      "                           'evaluation is correct',\n",
      "             'rating': '8: Top 50% of accepted papers, clear accept',\n",
      "             'review': '\\n'\n",
      "                       '########## Updated Review ##########\\n'\n",
      "                       '\\n'\n",
      "                       'The author(s) have presented a very good rebuttal, and '\n",
      "                       'I am impressed. My concerns have been addressed and my '\n",
      "                       'confusions have been clarified. To reflect this, I am '\n",
      "                       'raising my points to 8. It is a good paper, job well '\n",
      "                       'done. I enthusiastically recommend acceptance. \\n'\n",
      "                       '\\n'\n",
      "                       '################################\\n'\n",
      "                       '\\n'\n",
      "                       'A key challenge that presents the deep learning '\n",
      "                       'community is that state-of-the-art solutions are '\n",
      "                       'oftentimes associated with unstable derivatives, '\n",
      "                       'compromising the robustness of the network. In this '\n",
      "                       'paper, the author(s) explore the problem of how to '\n",
      "                       'train a neural network with stable derivatives by '\n",
      "                       'expanding the linear region associated with training '\n",
      "                       'samples. \\n'\n",
      "                       '\\n'\n",
      "                       'The author(s) studied deep networks with piecewise '\n",
      "                       'linear activations, which allow them to derive lower '\n",
      "                       'bounds on the $l_p$ margin with provably stable '\n",
      "                       'derivatives. In the special case of $l_2$ metric, this '\n",
      "                       'bound is analytic, albeit rigid and non-smooth. To '\n",
      "                       'avoid associated computational issues, the author(s) '\n",
      "                       'borrowed an idea from transductive/semi-supervised SVM '\n",
      "                       '(TSVM) to derive a relaxed formulation. \\n'\n",
      "                       '\\n'\n",
      "                       'In general, I find this paper rather interesting and '\n",
      "                       'well written. However, I do have a few concerns and '\n",
      "                       'confusions as listed below: \\n'\n",
      "                       '\\n'\n",
      "                       'Major ones:\\n'\n",
      "                       '\\n'\n",
      "                       '- I would prefer some elaborations on why the '\n",
      "                       'relaxation proposed in Eqn (8) serves to encourage the '\n",
      "                       \"margin of L2 ball? What's the working mechanism or \"\n",
      "                       'heuristic behind this relaxation? This is supposedly '\n",
      "                       'one of the key techniques used in optimization, yet '\n",
      "                       'remains obscure. \\n'\n",
      "                       '\\n'\n",
      "                       '- On empirical gains, the author(s) claimed that '\n",
      "                       '\"about 30 times larger margins can be achieved by '\n",
      "                       'trading off 1% accuracy.\"  It seems that consistently '\n",
      "                       'yields inferior prediction accuracy. In my opinion, '\n",
      "                       'the author(s) failed to showcase the practical utility '\n",
      "                       'of their solution. A better job should be done to '\n",
      "                       'validate the claim `` The problem we tackle has '\n",
      "                       'implications to interpretability and transparency of '\n",
      "                       \"complex models. '' \\n\"\n",
      "                       '\\n'\n",
      "                       '- As always, gradient-based penalties suffer from '\n",
      "                       'heavy computational overhead. The final objectives '\n",
      "                       'derived in this paper (Eqn (7) & Eqn (9)) seem no '\n",
      "                       'exception to this, and perhaps even worse since the '\n",
      "                       'gradient is taken wrt each neuron. Could the author(s) '\n",
      "                       'provide statistics on empirical wallclock performance? '\n",
      "                       'How much drain does this extra gradient penalty impose '\n",
      "                       'on the training efficiency? \\n'\n",
      "                       '\\n'\n",
      "                       'Minor ones:\\n'\n",
      "                       '\\n'\n",
      "                       '- Just to clarify, does the | - | used in Eqn (9) for '\n",
      "                       '|I(x,\\\\gamma)|  denote counting measure? \\n'\n",
      "                       '\\n'\n",
      "                       '- I do not see the necessity of introducing Lemma 7 in '\n",
      "                       'the text. Please explain. \\n'\n",
      "                       '\\n'\n",
      "                       '- Lemma 8, ``... then any optimal solutions for the '\n",
      "                       \"problem is also optimal for Eqn (4). '' Do you mean \"\n",
      "                       \"``the following problem'' (Eqn (5))? \\n\",\n",
      "             'title': 'It seems to be an interesting problem but is the '\n",
      "                      'solution proposed practical?'},\n",
      " 'ddate': None,\n",
      " 'details': None,\n",
      " 'forum': 'SylCrnCcFX',\n",
      " 'id': 'rkXgdwg0X',\n",
      " 'invitation': 'ICLR.cc/2019/Conference/-/Paper1591/Official_Review/AnonReviewer2/Revision',\n",
      " 'mdate': None,\n",
      " 'nonreaders': [],\n",
      " 'number': 1,\n",
      " 'original': None,\n",
      " 'readers': ['everyone'],\n",
      " 'referent': 'H1eDBfaunm',\n",
      " 'replyto': 'SylCrnCcFX',\n",
      " 'signatures': ['ICLR.cc/2019/Conference/Paper1591/AnonReviewer2'],\n",
      " 'tcdate': 1542645739449,\n",
      " 'tmdate': 1542645739449,\n",
      " 'writers': ['ICLR.cc/2019/Conference/Paper1591/AnonReviewer2',\n",
      "             'ICLR.cc/2019/Conference']}\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|█████████▏                                    | 2/10 [00:19<01:20, 10.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SkgjyZRcKm H1xAH2RqK7 SysJWR5tm\n",
      "SkgjyZRcKm H1xAH2RqK7 B1UYZr9CX\n",
      "SkgjyZRcKm H1xAH2RqK7 BrBNIO_M15\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|█████████████▊                                | 3/10 [00:23<00:53,  7.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BJe1HWOtK7 HJeABnCqKQ ByJHWdFFQ\n",
      "BJe1HWOtK7 HJeABnCqKQ SkitUh_RX\n",
      "BJe1HWOtK7 HJeABnCqKQ rUqtZ_Ozkc\n",
      "{'cdate': 1541173479791,\n",
      " 'content': {'confidence': '4: The reviewer is confident but not absolutely '\n",
      "                           'certain that the evaluation is correct',\n",
      "             'rating': '4: Ok but not good enough - rejection',\n",
      "             'review': 'This paper presents an incremental extension to the '\n",
      "                       'Self-imitation paper by Oh, Junhyuk, et al. The '\n",
      "                       'previous paper combined self-imitation learning with '\n",
      "                       'actor-critic methods, and this paper directly '\n",
      "                       'integrates the idea into the generative adversarial '\n",
      "                       'imitation learning framework.\\n'\n",
      "                       '\\n'\n",
      "                       'I think the idea is interesting, but there remains '\n",
      "                       'some issues very unclear to me. In the algorithms, '\n",
      "                       'when updating the good trajectory buffer, it is said '\n",
      "                       '\"We define ‘good trajectories’ as any trajectories '\n",
      "                       'whose the discounted sum of rewards are higher than '\n",
      "                       'that of the policy\". What does \"that of the policy\" '\n",
      "                       'mean? How do you know the reward of the policy?\\n'\n",
      "                       '\\n'\n",
      "                       \"Second, without defining good trajectories, I don't \"\n",
      "                       'think Algorithm 1 would work. Algorithm ` 1 misses the '\n",
      "                       'part of how to update buffer B. After introducing '\n",
      "                       'their own algorithm, the author did not provide much '\n",
      "                       'solid proof or analysis for why this self-imitation '\n",
      "                       'learning works.\\n'\n",
      "                       '\\n'\n",
      "                       'In the experiment section, the author implemented '\n",
      "                       'GASIL for various applications and presented '\n",
      "                       'reasonable results and compared them with other '\n",
      "                       'methods. Nevertheless, without theoretical proof, it '\n",
      "                       'is hardly convincing that the results could be '\n",
      "                       'consistently reproduced instead of being merely '\n",
      "                       'accidental for some applications.\\n',\n",
      "             'title': 'combination of GAIL and self-imitation learning, but '\n",
      "                      'not convincing'},\n",
      " 'ddate': None,\n",
      " 'details': None,\n",
      " 'forum': 'HJeABnCqKQ',\n",
      " 'id': 'Sylx-lq3m',\n",
      " 'invitation': 'ICLR.cc/2019/Conference/-/Paper1589/Official_Review',\n",
      " 'mdate': None,\n",
      " 'nonreaders': [],\n",
      " 'number': 1,\n",
      " 'original': None,\n",
      " 'readers': ['everyone'],\n",
      " 'referent': 'Syelg-g92X',\n",
      " 'replyto': 'HJeABnCqKQ',\n",
      " 'signatures': ['ICLR.cc/2019/Conference/Paper1589/AnonReviewer1'],\n",
      " 'tcdate': 1541173479791,\n",
      " 'tmdate': 1541533008539,\n",
      " 'writers': ['ICLR.cc/2019/Conference']}\n",
      "{'cdate': 1543384260627,\n",
      " 'content': {'confidence': '4: The reviewer is confident but not absolutely '\n",
      "                           'certain that the evaluation is correct',\n",
      "             'rating': '5: Marginally below acceptance threshold',\n",
      "             'review': 'This paper presents an incremental extension to the '\n",
      "                       'Self-imitation paper by Oh, Junhyuk, et al. The '\n",
      "                       'previous paper combined self-imitation learning with '\n",
      "                       'actor-critic methods, and this paper directly '\n",
      "                       'integrates the idea into the generative adversarial '\n",
      "                       'imitation learning framework.\\n'\n",
      "                       '\\n'\n",
      "                       'I think the idea is interesting, but there remains '\n",
      "                       'some issues very unclear to me. In the algorithms, '\n",
      "                       'when updating the good trajectory buffer, it is said '\n",
      "                       '\"We define ‘good trajectories’ as any trajectories '\n",
      "                       'whose the discounted sum of rewards are higher than '\n",
      "                       'that of the policy\". What does \"that of the policy\" '\n",
      "                       'mean? How do you know the reward of the policy?\\n'\n",
      "                       '\\n'\n",
      "                       \"Second, without defining good trajectories, I don't \"\n",
      "                       'think Algorithm 1 would work. Algorithm ` 1 misses the '\n",
      "                       'part of how to update buffer B. After introducing '\n",
      "                       'their own algorithm, the author did not provide much '\n",
      "                       'solid proof or analysis for why this self-imitation '\n",
      "                       'learning works.\\n'\n",
      "                       '\\n'\n",
      "                       'In the experiment section, the author implemented '\n",
      "                       'GASIL for various applications and presented '\n",
      "                       'reasonable results and compared them with other '\n",
      "                       'methods. Nevertheless, without theoretical proof, it '\n",
      "                       'is hardly convincing that the results could be '\n",
      "                       'consistently reproduced instead of being merely '\n",
      "                       'accidental for some applications.\\n'\n",
      "                       '\\n'\n",
      "                       'Update:\\n'\n",
      "                       'The rebuttal resolves some of my concerns. However, I '\n",
      "                       'still think the contribution is incremental. The '\n",
      "                       'current version looks too heuristic, more theoretical '\n",
      "                       'analysis or inspirations need to be added.\\n',\n",
      "             'title': 'combination of GAIL and self-imitation learning, but '\n",
      "                      'not convincing'},\n",
      " 'ddate': None,\n",
      " 'details': None,\n",
      " 'forum': 'HJeABnCqKQ',\n",
      " 'id': 'BJ6anjiR7',\n",
      " 'invitation': 'ICLR.cc/2019/Conference/-/Paper1589/Official_Review/AnonReviewer1/Revision',\n",
      " 'mdate': None,\n",
      " 'nonreaders': [],\n",
      " 'number': 1,\n",
      " 'original': None,\n",
      " 'readers': ['everyone'],\n",
      " 'referent': 'Syelg-g92X',\n",
      " 'replyto': 'HJeABnCqKQ',\n",
      " 'signatures': ['ICLR.cc/2019/Conference/Paper1589/AnonReviewer1'],\n",
      " 'tcdate': 1543384260627,\n",
      " 'tmdate': 1543384260627,\n",
      " 'writers': ['ICLR.cc/2019/Conference/Paper1589/AnonReviewer1',\n",
      "             'ICLR.cc/2019/Conference']}\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|██████████████████▍                           | 4/10 [00:28<00:38,  6.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HyeS8-RqFX SyVpB2RqFX r1HU-A5Y7\n",
      "HyeS8-RqFX SyVpB2RqFX Sy4xxlcRm\n",
      "HyeS8-RqFX SyVpB2RqFX BkR_MeqRQ\n",
      "HyeS8-RqFX SyVpB2RqFX ryp-CmqRQ\n",
      "HyeS8-RqFX SyVpB2RqFX rJtEA75Rm\n",
      "HyeS8-RqFX SyVpB2RqFX S1Xn5DcAX\n",
      "HyeS8-RqFX SyVpB2RqFX HybKcu5CQ\n",
      "HyeS8-RqFX SyVpB2RqFX HJdn3O5R7\n",
      "HyeS8-RqFX SyVpB2RqFX SJ01A_5Cm\n",
      "HyeS8-RqFX SyVpB2RqFX rMfttddzJ9\n",
      "HyeS8-RqFX SyVpB2RqFX SVg38GBp3f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|███████████████████████                       | 5/10 [00:38<00:39,  7.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "S1gVAfRqtQ SJf6BhAqK7 HkV0GRcF7\n",
      "S1gVAfRqtQ SJf6BhAqK7 rJHKE6707\n",
      "S1gVAfRqtQ SJf6BhAqK7 rygRC1q07\n",
      "{'cdate': 1541201397788,\n",
      " 'content': {'confidence': '4: The reviewer is confident but not absolutely '\n",
      "                           'certain that the evaluation is correct',\n",
      "             'rating': '5: Marginally below acceptance threshold',\n",
      "             'review': 'Review Summary\\n'\n",
      "                       '--------------\\n'\n",
      "                       'While the focus on variadic learning is interesting, I '\n",
      "                       'think the present version of the paper needs far more '\n",
      "                       'presentational polish as well as algorithmic '\n",
      "                       'improvements before it is ready for ICLR. I think '\n",
      "                       'there is the potential for some neat ideas here and I '\n",
      "                       'hope the authors prepare stronger versions in the '\n",
      "                       'future. However, the current version is unfortunately '\n",
      "                       'not comprehensible or reproducible.\\n'\n",
      "                       '\\n'\n",
      "                       'Paper Summary\\n'\n",
      "                       '-------------\\n'\n",
      "                       '\\n'\n",
      "                       'The paper investigates developing an effective ML '\n",
      "                       'method for the \"variadic\" regime, where the method '\n",
      "                       'might be required to perform learning from few or many '\n",
      "                       'examples (shots) and few or many classes (ways). The '\n",
      "                       'term \"variadic\" comes from use in computer science for '\n",
      "                       'functions that can a flexible number of arguments. '\n",
      "                       'There may also be unlabeled data available in the few '\n",
      "                       'shot case, creating semi-supervised learning '\n",
      "                       'opportunities.\\n'\n",
      "                       '\\n'\n",
      "                       'The specific method proposed is called BANDE: Bayesian '\n",
      "                       'Nonparametric Deep Embedding. The idea is that each '\n",
      "                       \"data point's feature vector x_i is transformed into an \"\n",
      "                       'embedding vector h(x_i) using a neural network, and '\n",
      "                       'then clustering occurs in the embedding space via a '\n",
      "                       'single-pass of the DP-means algorithm (Kulis & Jordan '\n",
      "                       '2012). Each cluster is assumed to correspond to one '\n",
      "                       '\"class\" in the eventual classification problem, though '\n",
      "                       'each class might have multiple clusters (and thus be '\n",
      "                       'multi-modal).  \\n'\n",
      "                       '\\n'\n",
      "                       'Learning occurs in an episodic manner. After each '\n",
      "                       'episode (single-pass of DP-means), each point in a '\n",
      "                       'query set is embedded to its feature vector, then fed '\n",
      "                       \"into each cluster's Gaussian likelihoods to produce a \"\n",
      "                       'normalized cluster-assignment-probability vector that '\n",
      "                       'sums to one. This vector is then fed into a '\n",
      "                       \"cross-entropy loss, where the true class's nearest \"\n",
      "                       'cluster (largest probability value) is taken to be the '\n",
      "                       'true cluster. This loss is used to perform gradient '\n",
      "                       'updates of the embedding neural network.\\n'\n",
      "                       '\\n'\n",
      "                       'There is also a \"cumulative\" version of the method '\n",
      "                       'called BANDE-C. This version keeps track of cluster '\n",
      "                       'means from previous episodes and allows new episodes '\n",
      "                       'to be initialized with these.\\n'\n",
      "                       '\\n'\n",
      "                       'Experiments examine the proposed approach across image '\n",
      "                       'categorization tasks on Omniglot, mini-ImageNet, and '\n",
      "                       'CIFAR datasets.\\n'\n",
      "                       '\\n'\n",
      "                       '\\n'\n",
      "                       'Strengths\\n'\n",
      "                       '---------\\n'\n",
      "                       '* I like that many clusters are used for each true '\n",
      "                       'class label, which is better than rigid one-to-one '\n",
      "                       'assumptions.\\n'\n",
      "                       '\\n'\n",
      "                       '\\n'\n",
      "                       'Limitations\\n'\n",
      "                       '-----------\\n'\n",
      "                       '* Can only be used for classification, not regression\\n'\n",
      "                       '* The DP-means procedure does not account for the '\n",
      "                       'cluster-specific variance information that is used at '\n",
      "                       'other steps of the algorithm\\n'\n",
      "                       '\\n'\n",
      "                       '\\n'\n",
      "                       'Significance and Originality\\n'\n",
      "                       '----------------------------\\n'\n",
      "                       'To me, the method appears original. Any method that '\n",
      "                       'could really succeed across various variadic settings '\n",
      "                       'would be significant.\\n'\n",
      "                       '\\n'\n",
      "                       '\\n'\n",
      "                       '\\n'\n",
      "                       'Presentation Concerns\\n'\n",
      "                       '---------------------\\n'\n",
      "                       '\\n'\n",
      "                       'I have serious concerns about the presentation quality '\n",
      "                       'of this paper. Each section needs careful '\n",
      "                       'reorganization as well as rewording.\\n'\n",
      "                       '\\n'\n",
      "                       '## P1: Algo. 1 contains numerous omissions that make '\n",
      "                       'it as written not correct.\\n'\n",
      "                       '\\n'\n",
      "                       '* the number of clusters count variable \"n\" is not '\n",
      "                       'updated anywhere. As writting this algo can only '\n",
      "                       'update one extra cluster beyond the original n.\\n'\n",
      "                       '* the variable \"c\" is unbound in the else clause. You '\n",
      "                       'need a line that clarifies that c = argmin_{c in 1 ... '\n",
      "                       'n} d_ic\\n'\n",
      "                       '\\n'\n",
      "                       'Would be careful about saying that \"a single pass is '\n",
      "                       'sufficient\"... you have *chosen* to do only one pass. '\n",
      "                       'When doing k-means, we could also make this choice. '\n",
      "                       'Certainly the DP-means objective could keep improving '\n",
      "                       'with multiple passes.\\n'\n",
      "                       '\\n'\n",
      "                       '## P2: Many figures and tables lack appropriate '\n",
      "                       'captions/labels\\n'\n",
      "                       '\\n'\n",
      "                       'Table 1: What metric is reported? Accuracy percentage? '\n",
      "                       'Not obvious from title/caption. Should also make very '\n",
      "                       'clear here how much labeled data was used.\\n'\n",
      "                       '\\n'\n",
      "                       'Table 2: What metric is reported? Accuracy percentage? '\n",
      "                       'Not obvious from title/caption. Should also make how '\n",
      "                       'many labeled and unlabeled examples were used easier '\n",
      "                       'to find.\\n'\n",
      "                       '\\n'\n",
      "                       '## P3: Descriptions of episodic learning and overall '\n",
      "                       'algorithm clarity\\n'\n",
      "                       '\\n'\n",
      "                       'Readers unfamiliar with episodic learning are not '\n",
      "                       'helped with the limited coverage provided here in 3.1 '\n",
      "                       'and 3.2. When exactly is the \"support\" set used and '\n",
      "                       'the \"query\" set used? How do unlabeled points get used '\n",
      "                       '(both support and query appear fully labeled)? What is '\n",
      "                       'n? What is k? What is T? Why are some points in Q '\n",
      "                       'denoted with apostrophes but not others? Providing a '\n",
      "                       'more formal step-by-step description (perhaps with '\n",
      "                       'pseudocode) will be crucial.\\n'\n",
      "                       '\\n'\n",
      "                       'In Sec. 3.2, the paragraph that starts with \"The loss '\n",
      "                       'is defined\" is very hard to read and parse. I suggest '\n",
      "                       'adding math to formally define the loss with '\n",
      "                       'equations. What parameters are being optimized? Which '\n",
      "                       'ones are fixed?\\n'\n",
      "                       '\\n'\n",
      "                       'Additionally, in Sec. 3.2: \"computed in the same way '\n",
      "                       'as standard prototypical networks\"... what is the '\n",
      "                       'procedure exactly? If your method relies on a '\n",
      "                       'procedure, you should specify it in this paper and not '\n",
      "                       'make readers guess or lookup a procedure elsewhere.\\n'\n",
      "                       '\\n'\n",
      "                       '\\n'\n",
      "                       '## P4: Many steps of the algorithm are not detailed\\n'\n",
      "                       '\\n'\n",
      "                       'The paper claims to set \\\\lambda using a technique '\n",
      "                       'from another paper, but does not summarize this '\n",
      "                       'technique. This makes things nearly impossible to '\n",
      "                       'reproduce. Please add such details in the appendix.\\n'\n",
      "                       '\\n'\n",
      "                       'Major Technical Concerns\\n'\n",
      "                       '------------------------\\n'\n",
      "                       '\\n'\n",
      "                       '## Alg. 1 concerns: Requires two (not one) passes and '\n",
      "                       'mixes hard and soft assingments and different variance '\n",
      "                       'assumptions awkwardly\\n'\n",
      "                       '\\n'\n",
      "                       'The BANDE algorithm (Alg. 1) has some unjustified '\n",
      "                       'properties. Hard assignment decisions which assume '\n",
      "                       'vanishing variances are used to find a closest '\n",
      "                       'cluster, but then later soft assignments with non-zero '\n",
      "                       'variances are used. This is a bit heuristic and lacks '\n",
      "                       'justification... why not use soft assignment '\n",
      "                       'throughout? The DP means procedure is derived from a '\n",
      "                       'specific objective function that assumes hard '\n",
      "                       'assignment. Seems weird to use it for convenience and '\n",
      "                       'then discard instead of coming up with the small fix '\n",
      "                       'that would make soft assignment consistent '\n",
      "                       'throughout.\\n'\n",
      "                       '\\n'\n",
      "                       'Furthermore, The authors claim it is a one pass '\n",
      "                       'algorithm, but in fact as written in Alg. 1 it seems '\n",
      "                       'to require two passes: the first pass keeps an '\n",
      "                       'original set of cluster centers fixed and then creates '\n",
      "                       \"new centers whenever an example's distance to the \"\n",
      "                       'closest center exceeds \\\\lambda. But then, the *soft* '\n",
      "                       'assignment step that updates \"z\" requires again the '\n",
      "                       'distance from each point to all centers be computed, '\n",
      "                       'which requires another pass (since some new clusters '\n",
      "                       'may exist which did not when the point was first '\n",
      "                       'visited). While the new soft values will be close to '\n",
      "                       'zero, they will not be *exactly* zero, and thus they '\n",
      "                       'matter. \\n'\n",
      "                       '\\n'\n",
      "                       '## Unclear if/how cluster-specific variance parameters '\n",
      "                       'learned\\n'\n",
      "                       '\\n'\n",
      "                       'From the text on top of page 4, it seems that the '\n",
      "                       'paper assumes that there exist cluster-specific '\n",
      "                       'variances \\\\sigma_c. However, these are not mentioned '\n",
      "                       'elsewhere, only a general (not cluster-specific) label '\n",
      "                       'variance \\\\sigma and fixed unlabeled variance sigma_u '\n",
      "                       'are used.\\n'\n",
      "                       '\\n'\n",
      "                       '## Experiments lack comparison to internal baselines\\n'\n",
      "                       '\\n'\n",
      "                       \"The paper doesn't evaluate sensitivity to key fixed \"\n",
      "                       'hyperparameters (e.g. \\\\alpha, \\\\lambda) or compare '\n",
      "                       'variants of their approach (with and without soft '\n",
      "                       'clustering step, with and without multimodality via '\n",
      "                       'DP-means). It is difficult to tell which design '\n",
      "                       'choices of the method are most crucial.\\n',\n",
      "             'title': 'Hard to read and relies on unjustified, shifting '\n",
      "                      'assumptions '},\n",
      " 'ddate': None,\n",
      " 'details': None,\n",
      " 'forum': 'SJf6BhAqK7',\n",
      " 'id': 'SJ0eRL9hm',\n",
      " 'invitation': 'ICLR.cc/2019/Conference/-/Paper1587/Official_Review',\n",
      " 'mdate': None,\n",
      " 'nonreaders': [],\n",
      " 'number': 3,\n",
      " 'original': None,\n",
      " 'readers': ['everyone'],\n",
      " 'referent': 'S1xCeCI9nm',\n",
      " 'replyto': 'SJf6BhAqK7',\n",
      " 'signatures': ['ICLR.cc/2019/Conference/Paper1587/AnonReviewer2'],\n",
      " 'tcdate': 1541201397788,\n",
      " 'tmdate': 1541533010888,\n",
      " 'writers': ['ICLR.cc/2019/Conference']}\n",
      "{'cdate': 1543172423441,\n",
      " 'content': {'confidence': '4: The reviewer is confident but not absolutely '\n",
      "                           'certain that the evaluation is correct',\n",
      "             'rating': '5: Marginally below acceptance threshold',\n",
      "             'review': 'Update after Author Rebuttal\\n'\n",
      "                       '--------------\\n'\n",
      "                       \"After reading the rebuttal, I'm pleased that the \"\n",
      "                       'authors have made significant revisions, but I still '\n",
      "                       'think more work is needed. The \"hard/soft\" hybrid '\n",
      "                       \"approach still lacks justification and perhaps wasn't \"\n",
      "                       'compared to a soft/soft approach in a fair and '\n",
      "                       'fully-correct way (see detailed reply to authors). I '\n",
      "                       'also appreciate the efforts on revising clarity, but '\n",
      "                       'still find many clarity issues in the newest version '\n",
      "                       'that make the method hard to understand let alone '\n",
      "                       'reproduce. I thus stand by my rating of \"borderline '\n",
      "                       'rejection\" and urge the authors to prepare significant '\n",
      "                       'revisions for a future venue that avoid hybrids of '\n",
      "                       'hard/soft probabilities without justification. \\n'\n",
      "                       '\\n'\n",
      "                       '(Original review text below. Detailed replies to '\n",
      "                       'authors are in posts below their responses).\\n'\n",
      "                       '\\n'\n",
      "                       'Review Summary\\n'\n",
      "                       '--------------\\n'\n",
      "                       'While the focus on variadic learning is interesting, I '\n",
      "                       'think the present version of the paper needs far more '\n",
      "                       'presentational polish as well as algorithmic '\n",
      "                       'improvements before it is ready for ICLR. I think '\n",
      "                       'there is the potential for some neat ideas here and I '\n",
      "                       'hope the authors prepare stronger versions in the '\n",
      "                       'future. However, the current version is unfortunately '\n",
      "                       'not comprehensible or reproducible.\\n'\n",
      "                       '\\n'\n",
      "                       'Paper Summary\\n'\n",
      "                       '-------------\\n'\n",
      "                       '\\n'\n",
      "                       'The paper investigates developing an effective ML '\n",
      "                       'method for the \"variadic\" regime, where the method '\n",
      "                       'might be required to perform learning from few or many '\n",
      "                       'examples (shots) and few or many classes (ways). The '\n",
      "                       'term \"variadic\" comes from use in computer science for '\n",
      "                       'functions that can a flexible number of arguments. '\n",
      "                       'There may also be unlabeled data available in the few '\n",
      "                       'shot case, creating semi-supervised learning '\n",
      "                       'opportunities.\\n'\n",
      "                       '\\n'\n",
      "                       'The specific method proposed is called BANDE: Bayesian '\n",
      "                       'Nonparametric Deep Embedding. The idea is that each '\n",
      "                       \"data point's feature vector x_i is transformed into an \"\n",
      "                       'embedding vector h(x_i) using a neural network, and '\n",
      "                       'then clustering occurs in the embedding space via a '\n",
      "                       'single-pass of the DP-means algorithm (Kulis & Jordan '\n",
      "                       '2012). Each cluster is assumed to correspond to one '\n",
      "                       '\"class\" in the eventual classification problem, though '\n",
      "                       'each class might have multiple clusters (and thus be '\n",
      "                       'multi-modal).  \\n'\n",
      "                       '\\n'\n",
      "                       'Learning occurs in an episodic manner. After each '\n",
      "                       'episode (single-pass of DP-means), each point in a '\n",
      "                       'query set is embedded to its feature vector, then fed '\n",
      "                       \"into each cluster's Gaussian likelihoods to produce a \"\n",
      "                       'normalized cluster-assignment-probability vector that '\n",
      "                       'sums to one. This vector is then fed into a '\n",
      "                       \"cross-entropy loss, where the true class's nearest \"\n",
      "                       'cluster (largest probability value) is taken to be the '\n",
      "                       'true cluster. This loss is used to perform gradient '\n",
      "                       'updates of the embedding neural network.\\n'\n",
      "                       '\\n'\n",
      "                       'There is also a \"cumulative\" version of the method '\n",
      "                       'called BANDE-C. This version keeps track of cluster '\n",
      "                       'means from previous episodes and allows new episodes '\n",
      "                       'to be initialized with these.\\n'\n",
      "                       '\\n'\n",
      "                       'Experiments examine the proposed approach across image '\n",
      "                       'categorization tasks on Omniglot, mini-ImageNet, and '\n",
      "                       'CIFAR datasets.\\n'\n",
      "                       '\\n'\n",
      "                       '\\n'\n",
      "                       'Strengths\\n'\n",
      "                       '---------\\n'\n",
      "                       '* I like that many clusters are used for each true '\n",
      "                       'class label, which is better than rigid one-to-one '\n",
      "                       'assumptions.\\n'\n",
      "                       '\\n'\n",
      "                       '\\n'\n",
      "                       'Limitations\\n'\n",
      "                       '-----------\\n'\n",
      "                       '* Can only be used for classification, not regression\\n'\n",
      "                       '* The DP-means procedure does not account for the '\n",
      "                       'cluster-specific variance information that is used at '\n",
      "                       'other steps of the algorithm\\n'\n",
      "                       '\\n'\n",
      "                       '\\n'\n",
      "                       'Significance and Originality\\n'\n",
      "                       '----------------------------\\n'\n",
      "                       'To me, the method appears original. Any method that '\n",
      "                       'could really succeed across various variadic settings '\n",
      "                       'would be significant.\\n'\n",
      "                       '\\n'\n",
      "                       '\\n'\n",
      "                       '\\n'\n",
      "                       'Presentation Concerns\\n'\n",
      "                       '---------------------\\n'\n",
      "                       '\\n'\n",
      "                       'I have serious concerns about the presentation quality '\n",
      "                       'of this paper. Each section needs careful '\n",
      "                       'reorganization as well as rewording.\\n'\n",
      "                       '\\n'\n",
      "                       '## P1: Algo. 1 contains numerous omissions that make '\n",
      "                       'it as written not correct.\\n'\n",
      "                       '\\n'\n",
      "                       '* the number of clusters count variable \"n\" is not '\n",
      "                       'updated anywhere. As writting this algo can only '\n",
      "                       'update one extra cluster beyond the original n.\\n'\n",
      "                       '* the variable \"c\" is unbound in the else clause. You '\n",
      "                       'need a line that clarifies that c = argmin_{c in 1 ... '\n",
      "                       'n} d_ic\\n'\n",
      "                       '\\n'\n",
      "                       'Would be careful about saying that \"a single pass is '\n",
      "                       'sufficient\"... you have *chosen* to do only one pass. '\n",
      "                       'When doing k-means, we could also make this choice. '\n",
      "                       'Certainly the DP-means objective could keep improving '\n",
      "                       'with multiple passes.\\n'\n",
      "                       '\\n'\n",
      "                       '## P2: Many figures and tables lack appropriate '\n",
      "                       'captions/labels\\n'\n",
      "                       '\\n'\n",
      "                       'Table 1: What metric is reported? Accuracy percentage? '\n",
      "                       'Not obvious from title/caption. Should also make very '\n",
      "                       'clear here how much labeled data was used.\\n'\n",
      "                       '\\n'\n",
      "                       'Table 2: What metric is reported? Accuracy percentage? '\n",
      "                       'Not obvious from title/caption. Should also make how '\n",
      "                       'many labeled and unlabeled examples were used easier '\n",
      "                       'to find.\\n'\n",
      "                       '\\n'\n",
      "                       '## P3: Descriptions of episodic learning and overall '\n",
      "                       'algorithm clarity\\n'\n",
      "                       '\\n'\n",
      "                       'Readers unfamiliar with episodic learning are not '\n",
      "                       'helped with the limited coverage provided here in 3.1 '\n",
      "                       'and 3.2. When exactly is the \"support\" set used and '\n",
      "                       'the \"query\" set used? How do unlabeled points get used '\n",
      "                       '(both support and query appear fully labeled)? What is '\n",
      "                       'n? What is k? What is T? Why are some points in Q '\n",
      "                       'denoted with apostrophes but not others? Providing a '\n",
      "                       'more formal step-by-step description (perhaps with '\n",
      "                       'pseudocode) will be crucial.\\n'\n",
      "                       '\\n'\n",
      "                       'In Sec. 3.2, the paragraph that starts with \"The loss '\n",
      "                       'is defined\" is very hard to read and parse. I suggest '\n",
      "                       'adding math to formally define the loss with '\n",
      "                       'equations. What parameters are being optimized? Which '\n",
      "                       'ones are fixed?\\n'\n",
      "                       '\\n'\n",
      "                       'Additionally, in Sec. 3.2: \"computed in the same way '\n",
      "                       'as standard prototypical networks\"... what is the '\n",
      "                       'procedure exactly? If your method relies on a '\n",
      "                       'procedure, you should specify it in this paper and not '\n",
      "                       'make readers guess or lookup a procedure elsewhere.\\n'\n",
      "                       '\\n'\n",
      "                       '\\n'\n",
      "                       '## P4: Many steps of the algorithm are not detailed\\n'\n",
      "                       '\\n'\n",
      "                       'The paper claims to set \\\\lambda using a technique '\n",
      "                       'from another paper, but does not summarize this '\n",
      "                       'technique. This makes things nearly impossible to '\n",
      "                       'reproduce. Please add such details in the appendix.\\n'\n",
      "                       '\\n'\n",
      "                       'Major Technical Concerns\\n'\n",
      "                       '------------------------\\n'\n",
      "                       '\\n'\n",
      "                       '## Alg. 1 concerns: Requires two (not one) passes and '\n",
      "                       'mixes hard and soft assingments and different variance '\n",
      "                       'assumptions awkwardly\\n'\n",
      "                       '\\n'\n",
      "                       'The BANDE algorithm (Alg. 1) has some unjustified '\n",
      "                       'properties. Hard assignment decisions which assume '\n",
      "                       'vanishing variances are used to find a closest '\n",
      "                       'cluster, but then later soft assignments with non-zero '\n",
      "                       'variances are used. This is a bit heuristic and lacks '\n",
      "                       'justification... why not use soft assignment '\n",
      "                       'throughout? The DP means procedure is derived from a '\n",
      "                       'specific objective function that assumes hard '\n",
      "                       'assignment. Seems weird to use it for convenience and '\n",
      "                       'then discard instead of coming up with the small fix '\n",
      "                       'that would make soft assignment consistent '\n",
      "                       'throughout.\\n'\n",
      "                       '\\n'\n",
      "                       'Furthermore, The authors claim it is a one pass '\n",
      "                       'algorithm, but in fact as written in Alg. 1 it seems '\n",
      "                       'to require two passes: the first pass keeps an '\n",
      "                       'original set of cluster centers fixed and then creates '\n",
      "                       \"new centers whenever an example's distance to the \"\n",
      "                       'closest center exceeds \\\\lambda. But then, the *soft* '\n",
      "                       'assignment step that updates \"z\" requires again the '\n",
      "                       'distance from each point to all centers be computed, '\n",
      "                       'which requires another pass (since some new clusters '\n",
      "                       'may exist which did not when the point was first '\n",
      "                       'visited). While the new soft values will be close to '\n",
      "                       'zero, they will not be *exactly* zero, and thus they '\n",
      "                       'matter. \\n'\n",
      "                       '\\n'\n",
      "                       '## Unclear if/how cluster-specific variance parameters '\n",
      "                       'learned\\n'\n",
      "                       '\\n'\n",
      "                       'From the text on top of page 4, it seems that the '\n",
      "                       'paper assumes that there exist cluster-specific '\n",
      "                       'variances \\\\sigma_c. However, these are not mentioned '\n",
      "                       'elsewhere, only a general (not cluster-specific) label '\n",
      "                       'variance \\\\sigma and fixed unlabeled variance sigma_u '\n",
      "                       'are used.\\n'\n",
      "                       '\\n'\n",
      "                       '## Experiments lack comparison to internal baselines\\n'\n",
      "                       '\\n'\n",
      "                       \"The paper doesn't evaluate sensitivity to key fixed \"\n",
      "                       'hyperparameters (e.g. \\\\alpha, \\\\lambda) or compare '\n",
      "                       'variants of their approach (with and without soft '\n",
      "                       'clustering step, with and without multimodality via '\n",
      "                       'DP-means). It is difficult to tell which design '\n",
      "                       'choices of the method are most crucial.\\n',\n",
      "             'title': 'Hard to read and relies on unjustified, shifting '\n",
      "                      'assumptions '},\n",
      " 'ddate': None,\n",
      " 'details': None,\n",
      " 'forum': 'SJf6BhAqK7',\n",
      " 'id': 'rkkLbu_RQ',\n",
      " 'invitation': 'ICLR.cc/2019/Conference/-/Paper1587/Official_Review/AnonReviewer2/Revision',\n",
      " 'mdate': None,\n",
      " 'nonreaders': [],\n",
      " 'number': 1,\n",
      " 'original': None,\n",
      " 'readers': ['everyone'],\n",
      " 'referent': 'S1xCeCI9nm',\n",
      " 'replyto': 'SJf6BhAqK7',\n",
      " 'signatures': ['ICLR.cc/2019/Conference/Paper1587/AnonReviewer2'],\n",
      " 'tcdate': 1543172423441,\n",
      " 'tmdate': 1543172423441,\n",
      " 'writers': ['ICLR.cc/2019/Conference/Paper1587/AnonReviewer2',\n",
      "             'ICLR.cc/2019/Conference']}\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|███████████████████████████▌                  | 6/10 [00:44<00:28,  7.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rklzyMA5FQ H1faSn0qY7 HJQZM7GAQ\n",
      "rklzyMA5FQ H1faSn0qY7 BkGkGRqtQ\n",
      "rklzyMA5FQ H1faSn0qY7 HJ_1aTN0m\n",
      "rklzyMA5FQ H1faSn0qY7 r1eKl0EAQ\n",
      "rklzyMA5FQ H1faSn0qY7 r89l_uuG15\n",
      "{'cdate': 1541177275996,\n",
      " 'content': {'confidence': '2: The reviewer is willing to defend the '\n",
      "                           'evaluation, but it is quite likely that the '\n",
      "                           'reviewer did not understand central parts of the '\n",
      "                           'paper',\n",
      "             'rating': '5: Marginally below acceptance threshold',\n",
      "             'review': 'In this paper the authors propose DL2 a system for '\n",
      "                       'training and querying neural networks with logical '\n",
      "                       'constraints\\n'\n",
      "                       '\\n'\n",
      "                       'The proposed approach is intriguing but in my humble '\n",
      "                       'opinion the presentation of the paper could be '\n",
      "                       'improved. Indeed I think that the paper is bit too '\n",
      "                       'hard to follow. \\n'\n",
      "                       'The example at page 2 is not clearly explained.\\n'\n",
      "                       '\\n'\n",
      "                       'In Equation 1 the relationship between constants S_i '\n",
      "                       'and the variables z is not clear. Is each S_i an '\n",
      "                       'assignment to z?\\n'\n",
      "                       '\\n'\n",
      "                       'I do not understand the step from Eq. 4 to Eq. 6. Why '\n",
      "                       'arg min becomes min?\\n'\n",
      "                       '\\n'\n",
      "                       'At page 4 the authors state \"we sometimes write a '\n",
      "                       'predicate \\\\phi to denote its indicator function '\n",
      "                       '1_\\\\phi\". I’m a bit confused here, when the indicator '\n",
      "                       'function is used in equations 1-6?\\n'\n",
      "                       '\\n'\n",
      "                       'What kind of architecture is used for implementing '\n",
      "                       'DL2? Is a feedforward network used? How many layers '\n",
      "                       'does it have? How many neurons for each layer? No '\n",
      "                       'information about it is provided by authors.\\n'\n",
      "                       '\\n'\n",
      "                       'It is not clear to me why DL2/training is implemented '\n",
      "                       'in PyTorch and DL2/querying in TensorFlow. Are those '\n",
      "                       'two separate systems? And why implementing them using '\n",
      "                       'different frameworks?\\n'\n",
      "                       '\\n'\n",
      "                       'In conclusion, I’m a bit insecure about the rating to '\n",
      "                       'give to this paper, the system seems interesting, but '\n",
      "                       'several part are not clear to me.\\n'\n",
      "                       '\\n'\n",
      "                       '[Minor comments]\\n'\n",
      "                       'It seems strange to me to use the notation L_inf '\n",
      "                       'instead of B_\\\\epsilon to denote a ball.\\n'\n",
      "                       '\\n'\n",
      "                       'In theorem 1. \\\\delta is a constant, right? It seems '\n",
      "                       'strange to me to have a limit over a constant.',\n",
      "             'title': 'Review'},\n",
      " 'ddate': None,\n",
      " 'details': None,\n",
      " 'forum': 'H1faSn0qY7',\n",
      " 'id': 'ByETkbc37',\n",
      " 'invitation': 'ICLR.cc/2019/Conference/-/Paper1586/Official_Review',\n",
      " 'mdate': None,\n",
      " 'nonreaders': [],\n",
      " 'number': 1,\n",
      " 'original': None,\n",
      " 'readers': ['everyone'],\n",
      " 'referent': 'HJx46y-q37',\n",
      " 'replyto': 'H1faSn0qY7',\n",
      " 'signatures': ['ICLR.cc/2019/Conference/Paper1586/AnonReviewer1'],\n",
      " 'tcdate': 1541177275996,\n",
      " 'tmdate': 1541533011357,\n",
      " 'writers': ['ICLR.cc/2019/Conference']}\n",
      "{'cdate': 1541603732338,\n",
      " 'content': {'confidence': '2: The reviewer is willing to defend the '\n",
      "                           'evaluation, but it is quite likely that the '\n",
      "                           'reviewer did not understand central parts of the '\n",
      "                           'paper',\n",
      "             'rating': '5: Marginally below acceptance threshold',\n",
      "             'review': 'In this paper the authors propose DL2 a system for '\n",
      "                       'training and querying neural networks with logical '\n",
      "                       'constraints\\n'\n",
      "                       '\\n'\n",
      "                       'The proposed approach is intriguing but in my humble '\n",
      "                       'opinion the presentation of the paper could be '\n",
      "                       'improved. Indeed I think that the paper is bit too '\n",
      "                       'hard to follow. \\n'\n",
      "                       'The example at page 2 is not clearly explained.\\n'\n",
      "                       '\\n'\n",
      "                       'In Equation 1 the relationship between constants S_i '\n",
      "                       'and the variables z is not clear. Is each S_i an '\n",
      "                       'assignment to z?\\n'\n",
      "                       '\\n'\n",
      "                       'I do not understand the step from Eq. 4 to Eq. 6. Why '\n",
      "                       'does arg min become min?\\n'\n",
      "                       '\\n'\n",
      "                       'At page 4 the authors state \"we sometimes write a '\n",
      "                       'predicate \\\\phi to denote its indicator function '\n",
      "                       '1_\\\\phi\". I’m a bit confused here, when the indicator '\n",
      "                       'function is used in equations 1-6?\\n'\n",
      "                       '\\n'\n",
      "                       'What kind of architecture is used for implementing '\n",
      "                       'DL2? Is a feedforward network used? How many layers '\n",
      "                       'does it have? How many neurons for each layer? No '\n",
      "                       'information about it is provided by authors.\\n'\n",
      "                       '\\n'\n",
      "                       'It is not clear to me why DL2/training is implemented '\n",
      "                       'in PyTorch and DL2/querying in TensorFlow. Are those '\n",
      "                       'two separate systems? And why implementing them using '\n",
      "                       'different frameworks?\\n'\n",
      "                       '\\n'\n",
      "                       'In conclusion, I’m a bit insecure about the rating to '\n",
      "                       'give to this paper, the system seems interesting, but '\n",
      "                       'several part are not clear to me.\\n'\n",
      "                       '\\n'\n",
      "                       '[Minor comments]\\n'\n",
      "                       'It seems strange to me to use the notation L_inf '\n",
      "                       'instead of B_\\\\epsilon to denote a ball.\\n'\n",
      "                       '\\n'\n",
      "                       'In theorem 1. \\\\delta is a constant, right? It seems '\n",
      "                       'strange to me to have a limit over a constant.',\n",
      "             'title': 'Review'},\n",
      " 'ddate': None,\n",
      " 'details': None,\n",
      " 'forum': 'H1faSn0qY7',\n",
      " 'id': 'S1h9WtepX',\n",
      " 'invitation': 'ICLR.cc/2019/Conference/-/Paper1586/Official_Review/AnonReviewer1/Revision',\n",
      " 'mdate': None,\n",
      " 'nonreaders': [],\n",
      " 'number': 1,\n",
      " 'original': None,\n",
      " 'readers': ['everyone'],\n",
      " 'referent': 'HJx46y-q37',\n",
      " 'replyto': 'H1faSn0qY7',\n",
      " 'signatures': ['ICLR.cc/2019/Conference/Paper1586/AnonReviewer1'],\n",
      " 'tcdate': 1541603732338,\n",
      " 'tmdate': 1541603732338,\n",
      " 'writers': ['ICLR.cc/2019/Conference/Paper1586/AnonReviewer1',\n",
      "             'ICLR.cc/2019/Conference']}\n",
      "{'cdate': 1541603840384,\n",
      " 'content': {'confidence': '2: The reviewer is willing to defend the '\n",
      "                           'evaluation, but it is quite likely that the '\n",
      "                           'reviewer did not understand central parts of the '\n",
      "                           'paper',\n",
      "             'rating': '5: Marginally below acceptance threshold',\n",
      "             'review': 'In this paper the authors propose DL2 a system for '\n",
      "                       'training and querying neural networks with logical '\n",
      "                       'constraints\\n'\n",
      "                       '\\n'\n",
      "                       'The proposed approach is intriguing but in my humble '\n",
      "                       'opinion the presentation of the paper could be '\n",
      "                       'improved. Indeed I think that the paper is bit too '\n",
      "                       'hard to follow. \\n'\n",
      "                       'The example at page 2 is not clearly explained.\\n'\n",
      "                       '\\n'\n",
      "                       'In Equation 1 the relationship between constants S_i '\n",
      "                       'and the variables z is not clear. Is each S_i an '\n",
      "                       'assignment to z?\\n'\n",
      "                       '\\n'\n",
      "                       'I do not understand the step from Eq. 4 to Eq. 6. Why '\n",
      "                       'does arg min become min?\\n'\n",
      "                       '\\n'\n",
      "                       'At page 4 the authors state \"we sometimes write a '\n",
      "                       'predicate \\\\phi to denote its indicator function '\n",
      "                       '1_\\\\phi\". I’m a bit confused here, when is the '\n",
      "                       'indicator function used in equations 1-6?\\n'\n",
      "                       '\\n'\n",
      "                       'What kind of architecture is used for implementing '\n",
      "                       'DL2? Is a feedforward network used? How many layers '\n",
      "                       'does it have? How many neurons for each layer? No '\n",
      "                       'information about it is provided by authors.\\n'\n",
      "                       '\\n'\n",
      "                       'It is not clear to me why DL2/training is implemented '\n",
      "                       'in PyTorch and DL2/querying in TensorFlow. Are those '\n",
      "                       'two separate systems? And why implementing them using '\n",
      "                       'different frameworks?\\n'\n",
      "                       '\\n'\n",
      "                       'In conclusion, I’m a bit insecure about the rating to '\n",
      "                       'give to this paper, the system seems interesting, but '\n",
      "                       'several part are not clear to me.\\n'\n",
      "                       '\\n'\n",
      "                       '[Minor comments]\\n'\n",
      "                       'It seems strange to me to use the notation L_inf '\n",
      "                       'instead of B_\\\\epsilon to denote a ball.\\n'\n",
      "                       '\\n'\n",
      "                       'In theorem 1. \\\\delta is a constant, right? It seems '\n",
      "                       'strange to me to have a limit over a constant.',\n",
      "             'title': 'Review'},\n",
      " 'ddate': None,\n",
      " 'details': None,\n",
      " 'forum': 'H1faSn0qY7',\n",
      " 'id': 'HyObGtep7',\n",
      " 'invitation': 'ICLR.cc/2019/Conference/-/Paper1586/Official_Review/AnonReviewer1/Revision',\n",
      " 'mdate': None,\n",
      " 'nonreaders': [],\n",
      " 'number': 2,\n",
      " 'original': None,\n",
      " 'readers': ['everyone'],\n",
      " 'referent': 'HJx46y-q37',\n",
      " 'replyto': 'H1faSn0qY7',\n",
      " 'signatures': ['ICLR.cc/2019/Conference/Paper1586/AnonReviewer1'],\n",
      " 'tcdate': 1541603840384,\n",
      " 'tmdate': 1541603840384,\n",
      " 'writers': ['ICLR.cc/2019/Conference/Paper1586/AnonReviewer1',\n",
      "             'ICLR.cc/2019/Conference']}\n",
      "{'cdate': 1544536395042,\n",
      " 'content': {'confidence': '2: The reviewer is willing to defend the '\n",
      "                           'evaluation, but it is quite likely that the '\n",
      "                           'reviewer did not understand central parts of the '\n",
      "                           'paper',\n",
      "             'rating': '6: Marginally above acceptance threshold',\n",
      "             'review': 'In this paper the authors propose DL2 a system for '\n",
      "                       'training and querying neural networks with logical '\n",
      "                       'constraints\\n'\n",
      "                       '\\n'\n",
      "                       'The proposed approach is intriguing but in my humble '\n",
      "                       'opinion the presentation of the paper could be '\n",
      "                       'improved. Indeed I think that the paper is bit too '\n",
      "                       'hard to follow. \\n'\n",
      "                       'The example at page 2 is not clearly explained.\\n'\n",
      "                       '\\n'\n",
      "                       'In Equation 1 the relationship between constants S_i '\n",
      "                       'and the variables z is not clear. Is each S_i an '\n",
      "                       'assignment to z?\\n'\n",
      "                       '\\n'\n",
      "                       'I do not understand the step from Eq. 4 to Eq. 6. Why '\n",
      "                       'does arg min become min?\\n'\n",
      "                       '\\n'\n",
      "                       'At page 4 the authors state \"we sometimes write a '\n",
      "                       'predicate \\\\phi to denote its indicator function '\n",
      "                       '1_\\\\phi\". I’m a bit confused here, when is the '\n",
      "                       'indicator function used in equations 1-6?\\n'\n",
      "                       '\\n'\n",
      "                       'What kind of architecture is used for implementing '\n",
      "                       'DL2? Is a feedforward network used? How many layers '\n",
      "                       'does it have? How many neurons for each layer? No '\n",
      "                       'information about it is provided by authors.\\n'\n",
      "                       '\\n'\n",
      "                       'It is not clear to me why DL2/training is implemented '\n",
      "                       'in PyTorch and DL2/querying in TensorFlow. Are those '\n",
      "                       'two separate systems? And why implementing them using '\n",
      "                       'different frameworks?\\n'\n",
      "                       '\\n'\n",
      "                       'In conclusion, I’m a bit insecure about the rating to '\n",
      "                       'give to this paper, the system seems interesting, but '\n",
      "                       'several part are not clear to me.\\n'\n",
      "                       '\\n'\n",
      "                       '[Minor comments]\\n'\n",
      "                       'It seems strange to me to use the notation L_inf '\n",
      "                       'instead of B_\\\\epsilon to denote a ball.\\n'\n",
      "                       '\\n'\n",
      "                       'In theorem 1. \\\\delta is a constant, right? It seems '\n",
      "                       'strange to me to have a limit over a constant.',\n",
      "             'title': 'Review'},\n",
      " 'ddate': None,\n",
      " 'details': None,\n",
      " 'forum': 'H1faSn0qY7',\n",
      " 'id': 'Hk78Wr6JE',\n",
      " 'invitation': 'ICLR.cc/2019/Conference/-/Paper1586/Official_Review/AnonReviewer1/Revision',\n",
      " 'mdate': None,\n",
      " 'nonreaders': [],\n",
      " 'number': 3,\n",
      " 'original': None,\n",
      " 'readers': ['everyone'],\n",
      " 'referent': 'HJx46y-q37',\n",
      " 'replyto': 'H1faSn0qY7',\n",
      " 'signatures': ['ICLR.cc/2019/Conference/Paper1586/AnonReviewer1'],\n",
      " 'tcdate': 1544536395042,\n",
      " 'tmdate': 1544536395042,\n",
      " 'writers': ['ICLR.cc/2019/Conference/Paper1586/AnonReviewer1',\n",
      "             'ICLR.cc/2019/Conference']}\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|████████████████████████████████▏             | 7/10 [00:49<00:19,  6.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1gEek2qKQ HJgTHnActQ SyEeyh9Fm\n",
      "S1gEek2qKQ HJgTHnActQ ry7dCV5AX\n",
      "S1gEek2qKQ HJgTHnActQ BWEBK__f1c\n",
      "{'cdate': 1541023719954,\n",
      " 'content': {'confidence': '4: The reviewer is confident but not absolutely '\n",
      "                           'certain that the evaluation is correct',\n",
      "             'rating': '4: Ok but not good enough - rejection',\n",
      "             'review': 'Summary--\\n'\n",
      "                       'The paper tries to address an issue existing in '\n",
      "                       'current image-to-image translation at the point that '\n",
      "                       'different regions of the image should be treated '\n",
      "                       'differently. In other word, background should not be '\n",
      "                       'transferred while only foreground of interest should '\n",
      "                       'be transferred. The paper propose to use '\n",
      "                       'co-segmentation to find the common areas to for image '\n",
      "                       'translation. It reports the proposed method works '\n",
      "                       'through experiments.\\n'\n",
      "                       '\\n'\n",
      "                       'There are several major concerns to be addressed '\n",
      "                       'before considering to publish.\\n'\n",
      "                       '\\n'\n",
      "                       '1) The paper says that \"For example, in a person’s '\n",
      "                       'facial image translation, if the exemplar image has '\n",
      "                       'two attributes, (1) a smiling expression and (2) a '\n",
      "                       'blonde hair, then both attributes have to be '\n",
      "                       'transferred with no other options\", but the model in '\n",
      "                       'the paper seems still incapable of transferring only '\n",
      "                       'one attribute. Perhaps an interactive transfer make '\n",
      "                       'more sense, while co-segmentation does not distinguish '\n",
      "                       'the part of interest to the user. Or training a '\n",
      "                       'semantic segmentation make more sense as the semantic '\n",
      "                       'segment can specify which region to transfer.\\n'\n",
      "                       '\\n'\n",
      "                       '2) As co-segmentation is proposed to \"capture the '\n",
      "                       'regions of a common object existing in multiple input '\n",
      "                       'images\", why does the co-segmentation network only '\n",
      "                       'capture the eye and mouth part in Figure 2 and 3, why '\n",
      "                       'does it capture the mouth of different shape and style '\n",
      "                       'in the third macro column in Figure 4 instead of eyes? '\n",
      "                       'How to train the co-segmentation module, what is the '\n",
      "                       'objective function? Why not using a semantic '\n",
      "                       'segmentation model?\\n'\n",
      "                       '\\n'\n",
      "                       '3) The \"domain-invariant content code\" and the \"style '\n",
      "                       'code\" seem rather subjective. Are there any principles '\n",
      "                       'to design content and style codes? In the experiments, '\n",
      "                       'it seems the paper considers five styles to transfer '\n",
      "                       'as shown in Table 1. Is the model easy to extend to '\n",
      "                       'novel styles for image translation?\\n'\n",
      "                       '\\n'\n",
      "                       '4) What does the pink color mean in the very '\n",
      "                       'bottom-left or top-right heatmap images in Figure 2? '\n",
      "                       'There is no pink color reference in the colorbar.\\n'\n",
      "                       '\\n'\n",
      "                       '5) Figure 5: Why there is similariy dark patterns on '\n",
      "                       'the mouth? Is it some manual manipulation for '\n",
      "                       'interactive transfer?\\n'\n",
      "                       '\\n'\n",
      "                       '6) Though it is always good to see the authors are '\n",
      "                       'willing to release code and models, it appears '\n",
      "                       'uncomfortable that github page noted in the abstract '\n",
      "                       'reveals the author information. Moreover, in the '\n",
      "                       'github page,\\n'\n",
      "                       'even though it says \"an example is example.ipynb\", the '\n",
      "                       'only ipynb file contains nothing informative and this '\n",
      "                       'makes reviewers feel cheated.\\n'\n",
      "                       '\\n'\n",
      "                       'Minor--\\n'\n",
      "                       'There are several typos, e.g., lightinig.',\n",
      "             'title': 'paper should be improved before publishing'},\n",
      " 'ddate': None,\n",
      " 'details': None,\n",
      " 'forum': 'HJgTHnActQ',\n",
      " 'id': 'H1gl_iwhX',\n",
      " 'invitation': 'ICLR.cc/2019/Conference/-/Paper1585/Official_Review',\n",
      " 'mdate': None,\n",
      " 'nonreaders': [],\n",
      " 'number': 2,\n",
      " 'original': None,\n",
      " 'readers': ['everyone'],\n",
      " 'referent': 'S1eex_jDhm',\n",
      " 'replyto': 'HJgTHnActQ',\n",
      " 'signatures': ['ICLR.cc/2019/Conference/Paper1585/AnonReviewer3'],\n",
      " 'tcdate': 1541023719954,\n",
      " 'tmdate': 1541533012311,\n",
      " 'writers': ['ICLR.cc/2019/Conference']}\n",
      "{'cdate': 1543650556386,\n",
      " 'content': {'confidence': '4: The reviewer is confident but not absolutely '\n",
      "                           'certain that the evaluation is correct',\n",
      "             'rating': '5: Marginally below acceptance threshold',\n",
      "             'review': 'Summary--\\n'\n",
      "                       'The paper tries to address an issue existing in '\n",
      "                       'current image-to-image translation at the point that '\n",
      "                       'different regions of the image should be treated '\n",
      "                       'differently. In other word, background should not be '\n",
      "                       'transferred while only foreground of interest should '\n",
      "                       'be transferred. The paper propose to use '\n",
      "                       'co-segmentation to find the common areas to for image '\n",
      "                       'translation. It reports the proposed method works '\n",
      "                       'through experiments.\\n'\n",
      "                       '\\n'\n",
      "                       'There are several major concerns to be addressed '\n",
      "                       'before considering to publish.\\n'\n",
      "                       '\\n'\n",
      "                       '1) The paper says that \"For example, in a person’s '\n",
      "                       'facial image translation, if the exemplar image has '\n",
      "                       'two attributes, (1) a smiling expression and (2) a '\n",
      "                       'blonde hair, then both attributes have to be '\n",
      "                       'transferred with no other options\", but the model in '\n",
      "                       'the paper seems still incapable of transferring only '\n",
      "                       'one attribute. Perhaps an interactive transfer make '\n",
      "                       'more sense, while co-segmentation does not distinguish '\n",
      "                       'the part of interest to the user. Or training a '\n",
      "                       'semantic segmentation make more sense as the semantic '\n",
      "                       'segment can specify which region to transfer.\\n'\n",
      "                       '\\n'\n",
      "                       '2) As co-segmentation is proposed to \"capture the '\n",
      "                       'regions of a common object existing in multiple input '\n",
      "                       'images\", why does the co-segmentation network only '\n",
      "                       'capture the eye and mouth part in Figure 2 and 3, why '\n",
      "                       'does it capture the mouth of different shape and style '\n",
      "                       'in the third macro column in Figure 4 instead of eyes? '\n",
      "                       'How to train the co-segmentation module, what is the '\n",
      "                       'objective function? Why not using a semantic '\n",
      "                       'segmentation model?\\n'\n",
      "                       '\\n'\n",
      "                       '3) The \"domain-invariant content code\" and the \"style '\n",
      "                       'code\" seem rather subjective. Are there any principles '\n",
      "                       'to design content and style codes? In the experiments, '\n",
      "                       'it seems the paper considers five styles to transfer '\n",
      "                       'as shown in Table 1. Is the model easy to extend to '\n",
      "                       'novel styles for image translation?\\n'\n",
      "                       '\\n'\n",
      "                       '4) What does the pink color mean in the very '\n",
      "                       'bottom-left or top-right heatmap images in Figure 2? '\n",
      "                       'There is no pink color reference in the colorbar.\\n'\n",
      "                       '\\n'\n",
      "                       '5) Figure 5: Why there is similariy dark patterns on '\n",
      "                       'the mouth? Is it some manual manipulation for '\n",
      "                       'interactive transfer?\\n'\n",
      "                       '\\n'\n",
      "                       '6) Though it is always good to see the authors are '\n",
      "                       'willing to release code and models, it appears '\n",
      "                       'uncomfortable that github page noted in the abstract '\n",
      "                       'reveals the author information. Moreover, in the '\n",
      "                       'github page,\\n'\n",
      "                       'even though it says \"an example is example.ipynb\", the '\n",
      "                       'only ipynb file contains nothing informative and this '\n",
      "                       'makes reviewers feel cheated.\\n'\n",
      "                       '\\n'\n",
      "                       'Minor--\\n'\n",
      "                       'There are several typos, e.g., lightinig.',\n",
      "             'title': 'paper should be improved before publishing'},\n",
      " 'ddate': None,\n",
      " 'details': None,\n",
      " 'forum': 'HJgTHnActQ',\n",
      " 'id': 'S14-a3kyE',\n",
      " 'invitation': 'ICLR.cc/2019/Conference/-/Paper1585/Official_Review/AnonReviewer3/Revision',\n",
      " 'mdate': None,\n",
      " 'nonreaders': [],\n",
      " 'number': 1,\n",
      " 'original': None,\n",
      " 'readers': ['everyone'],\n",
      " 'referent': 'S1eex_jDhm',\n",
      " 'replyto': 'HJgTHnActQ',\n",
      " 'signatures': ['ICLR.cc/2019/Conference/Paper1585/AnonReviewer3'],\n",
      " 'tcdate': 1543650556386,\n",
      " 'tmdate': 1543650556386,\n",
      " 'writers': ['ICLR.cc/2019/Conference/Paper1585/AnonReviewer3',\n",
      "             'ICLR.cc/2019/Conference']}\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████████████████████████████████▊         | 8/10 [00:54<00:12,  6.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ByxzSCs5K7 HylTBhA5tQ rJGBRocFm\n",
      "ByxzSCs5K7 HylTBhA5tQ BJT6KpfTQ\n",
      "ByxzSCs5K7 HylTBhA5tQ HJUM96M6Q\n",
      "ByxzSCs5K7 HylTBhA5tQ B1ncGoyAQ\n",
      "ByxzSCs5K7 HylTBhA5tQ Sk3cLIcAQ\n",
      "ByxzSCs5K7 HylTBhA5tQ B1F2VV5xV\n",
      "ByxzSCs5K7 HylTBhA5tQ BkE1GKcgN\n",
      "ByxzSCs5K7 HylTBhA5tQ HyWTqT9gE\n",
      "ByxzSCs5K7 HylTBhA5tQ SJuL7qV-V\n",
      "ByxzSCs5K7 HylTBhA5tQ Byo3z7sbN\n",
      "ByxzSCs5K7 HylTBhA5tQ BkpZGq3ZN\n",
      "ByxzSCs5K7 HylTBhA5tQ BJ64z53ZE\n",
      "ByxzSCs5K7 HylTBhA5tQ HJtIL9h-4\n",
      "{'cdate': 1540864380131,\n",
      " 'content': {'confidence': '4: The reviewer is confident but not absolutely '\n",
      "                           'certain that the evaluation is correct',\n",
      "             'rating': '5: Marginally below acceptance threshold',\n",
      "             'review': 'In this paper, the authors associated with the '\n",
      "                       'generalization gap of robust adversarial training with '\n",
      "                       'the distance between the test point and the manifold '\n",
      "                       \"of training data. A so-called 'blind-spot attack' is \"\n",
      "                       'proposed to show the weakness of robust adversarial '\n",
      "                       'training.  Although the paper contains interesting '\n",
      "                       'ideas and empirical results, I have several concerns '\n",
      "                       'about the current version. \\n'\n",
      "                       '\\n'\n",
      "                       'a) In the paper, the authors mentioned that \"This '\n",
      "                       'simple metric is non-parametric and we found that the '\n",
      "                       'results are not sensitive to the selection of k\". Can '\n",
      "                       'authors provide more details, e.g., empirical results, '\n",
      "                       'about it? What is its rationale?\\n'\n",
      "                       '\\n'\n",
      "                       'b) In the paper, \"We find that these blind-spots are '\n",
      "                       'prevalent and can be easily found without resorting to '\n",
      "                       'complex\\n'\n",
      "                       'generative models like in Song et al. (2018). For the '\n",
      "                       'MNIST dataset which Madry et al. (2018) demonstrate '\n",
      "                       'the strongest defense results so far, we propose a '\n",
      "                       'simple transformation to find the blind-spots in this '\n",
      "                       'model.\" Can authors provide empirical comparison '\n",
      "                       'between blind-spot attacks and the work by Song et al. '\n",
      "                       '(2018), e.g., attack success rate & distortion? \\n'\n",
      "                       '\\n'\n",
      "                       'c) The linear transformation x^\\\\prime = \\\\alpha x + '\n",
      "                       '\\\\beta yields a blind-spot attack which can defeat '\n",
      "                       'robust adversarial training. However, given the linear '\n",
      "                       'transformation, one can further modify the inner '\n",
      "                       'maximization (adv. example generation) in robust '\n",
      "                       'training framework so that the $\\\\ell_infty$ attack '\n",
      "                       'satisfies  max_{\\\\alpha, \\\\beta} f(\\\\alpha x + \\\\beta) '\n",
      "                       'subject to \\\\| \\\\alpha x + \\\\beta \\\\|\\\\leq \\\\epsilon. '\n",
      "                       'In this case, robust training framework can defend '\n",
      "                       'blind-spot attacks, right? I agree with the authors '\n",
      "                       'that the generalization error is due to the mismatch '\n",
      "                       'between training data and test data distribution, '\n",
      "                       'however, I am not convinced that blind-spot attacks '\n",
      "                       'are effective enough to robust training. \\n'\n",
      "                       '\\n'\n",
      "                       'd) \"Because we scale the image by a factor of \\\\alpha, '\n",
      "                       'we also set a stricter criterion of success, ..., '\n",
      "                       'perturbation must be less\\n'\n",
      "                       'than \\\\alpha \\\\epsilon to be counted as a successful '\n",
      "                       'attack.\" I did not get the point. Even if you have a '\n",
      "                       'scaling factor in x^\\\\prime = \\\\alpha x + \\\\beta, the '\n",
      "                       'universal perturbation rule should still be | x - '\n",
      "                       'x^\\\\prime  |_\\\\infty \\\\leq \\\\epsilon. The metric the '\n",
      "                       'authors used would result in a higher attack success '\n",
      "                       'rate, right? \\n',\n",
      "             'title': \"Reviewer's summery: interesting idea/findings but with \"\n",
      "                      'questions'},\n",
      " 'ddate': None,\n",
      " 'details': None,\n",
      " 'forum': 'HylTBhA5tQ',\n",
      " 'id': 'BkNtFNSh7',\n",
      " 'invitation': 'ICLR.cc/2019/Conference/-/Paper1584/Official_Review',\n",
      " 'mdate': None,\n",
      " 'nonreaders': [],\n",
      " 'number': 1,\n",
      " 'original': None,\n",
      " 'readers': ['everyone'],\n",
      " 'referent': 'H1gVYFVS3X',\n",
      " 'replyto': 'HylTBhA5tQ',\n",
      " 'signatures': ['ICLR.cc/2019/Conference/Paper1584/AnonReviewer3'],\n",
      " 'tcdate': 1540864380131,\n",
      " 'tmdate': 1541533013242,\n",
      " 'writers': ['ICLR.cc/2019/Conference']}\n",
      "{'cdate': 1543267789793,\n",
      " 'content': {'confidence': '4: The reviewer is confident but not absolutely '\n",
      "                           'certain that the evaluation is correct',\n",
      "             'rating': '6: Marginally above acceptance threshold',\n",
      "             'review': 'In this paper, the authors associated with the '\n",
      "                       'generalization gap of robust adversarial training with '\n",
      "                       'the distance between the test point and the manifold '\n",
      "                       \"of training data. A so-called 'blind-spot attack' is \"\n",
      "                       'proposed to show the weakness of robust adversarial '\n",
      "                       'training.  Although the paper contains interesting '\n",
      "                       'ideas and empirical results, I have several concerns '\n",
      "                       'about the current version. \\n'\n",
      "                       '\\n'\n",
      "                       'a) In the paper, the authors mentioned that \"This '\n",
      "                       'simple metric is non-parametric and we found that the '\n",
      "                       'results are not sensitive to the selection of k\". Can '\n",
      "                       'authors provide more details, e.g., empirical results, '\n",
      "                       'about it? What is its rationale?\\n'\n",
      "                       '\\n'\n",
      "                       'b) In the paper, \"We find that these blind-spots are '\n",
      "                       'prevalent and can be easily found without resorting to '\n",
      "                       'complex\\n'\n",
      "                       'generative models like in Song et al. (2018). For the '\n",
      "                       'MNIST dataset which Madry et al. (2018) demonstrate '\n",
      "                       'the strongest defense results so far, we propose a '\n",
      "                       'simple transformation to find the blind-spots in this '\n",
      "                       'model.\" Can authors provide empirical comparison '\n",
      "                       'between blind-spot attacks and the work by Song et al. '\n",
      "                       '(2018), e.g., attack success rate & distortion? \\n'\n",
      "                       '\\n'\n",
      "                       'c) The linear transformation x^\\\\prime = \\\\alpha x + '\n",
      "                       '\\\\beta yields a blind-spot attack which can defeat '\n",
      "                       'robust adversarial training. However, given the linear '\n",
      "                       'transformation, one can further modify the inner '\n",
      "                       'maximization (adv. example generation) in robust '\n",
      "                       'training framework so that the $\\\\ell_infty$ attack '\n",
      "                       'satisfies  max_{\\\\alpha, \\\\beta} f(\\\\alpha x + \\\\beta) '\n",
      "                       'subject to \\\\| \\\\alpha x + \\\\beta \\\\|\\\\leq \\\\epsilon. '\n",
      "                       'In this case, robust training framework can defend '\n",
      "                       'blind-spot attacks, right? I agree with the authors '\n",
      "                       'that the generalization error is due to the mismatch '\n",
      "                       'between training data and test data distribution, '\n",
      "                       'however, I am not convinced that blind-spot attacks '\n",
      "                       'are effective enough to robust training. \\n'\n",
      "                       '\\n'\n",
      "                       'd) \"Because we scale the image by a factor of \\\\alpha, '\n",
      "                       'we also set a stricter criterion of success, ..., '\n",
      "                       'perturbation must be less\\n'\n",
      "                       'than \\\\alpha \\\\epsilon to be counted as a successful '\n",
      "                       'attack.\" I did not get the point. Even if you have a '\n",
      "                       'scaling factor in x^\\\\prime = \\\\alpha x + \\\\beta, the '\n",
      "                       'universal perturbation rule should still be | x - '\n",
      "                       'x^\\\\prime  |_\\\\infty \\\\leq \\\\epsilon. The metric the '\n",
      "                       'authors used would result in a higher attack success '\n",
      "                       'rate, right? \\n',\n",
      "             'title': \"Reviewer's summery: interesting idea/findings but with \"\n",
      "                      'questions'},\n",
      " 'ddate': None,\n",
      " 'details': None,\n",
      " 'forum': 'HylTBhA5tQ',\n",
      " 'id': 'rkIRrkcCm',\n",
      " 'invitation': 'ICLR.cc/2019/Conference/-/Paper1584/Official_Review/AnonReviewer3/Revision',\n",
      " 'mdate': None,\n",
      " 'nonreaders': [],\n",
      " 'number': 1,\n",
      " 'original': None,\n",
      " 'readers': ['everyone'],\n",
      " 'referent': 'H1gVYFVS3X',\n",
      " 'replyto': 'HylTBhA5tQ',\n",
      " 'signatures': ['ICLR.cc/2019/Conference/Paper1584/AnonReviewer3'],\n",
      " 'tcdate': 1543267789793,\n",
      " 'tmdate': 1543267789793,\n",
      " 'writers': ['ICLR.cc/2019/Conference/Paper1584/AnonReviewer3',\n",
      "             'ICLR.cc/2019/Conference']}\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████████████████████████████████████▍    | 9/10 [01:04<00:07,  7.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HJlhT3T5K7 B1gTShAct7 H1hanTcKQ\n",
      "HJlhT3T5K7 B1gTShAct7 H1D0419Am\n",
      "HJlhT3T5K7 B1gTShAct7 SJ2DBlcCX\n",
      "HJlhT3T5K7 B1gTShAct7 BJbHkMqR7\n",
      "HJlhT3T5K7 B1gTShAct7 ByvlvH0B4\n",
      "HJlhT3T5K7 B1gTShAct7 BJRVurRBV\n",
      "HJlhT3T5K7 B1gTShAct7 Sk0JX8AHV\n",
      "HJlhT3T5K7 B1gTShAct7 H1XY9BfFE\n",
      "HJlhT3T5K7 B1gTShAct7 rkmbRChY4\n",
      "HJlhT3T5K7 B1gTShAct7 BkmS7QtiE\n",
      "HJlhT3T5K7 B1gTShAct7 H5zeuuGyc\n",
      "{'cdate': 1541369820403,\n",
      " 'content': {'confidence': '5: The reviewer is absolutely certain that the '\n",
      "                           'evaluation is correct and very familiar with the '\n",
      "                           'relevant literature',\n",
      "             'rating': '5: Marginally below acceptance threshold',\n",
      "             'review': 'The transfer/ interference perspective of lifelong '\n",
      "                       'learning is well motivated, and combining the '\n",
      "                       'meta-learning literature with the continual learning '\n",
      "                       'literature (applying reptile twice), even if seems '\n",
      "                       \"obvious, wasn't explored before. In addition, this \"\n",
      "                       'paper shows that a lot of gain can be obtained if one '\n",
      "                       'uses more randomized and representative memory '\n",
      "                       \"(reservoir sampling). However, I'm not entirely \"\n",
      "                       'convinced with the technical contributions and the '\n",
      "                       'analysis provided to support the claims in the paper, '\n",
      "                       'good enough for me to accept it in its current form. '\n",
      "                       \"Please find below my concerns and I'm more than happy \"\n",
      "                       'to change my mind if the answers are convincing.\\n'\n",
      "                       '\\n'\n",
      "                       'Main concerns:\\n'\n",
      "                       '\\n'\n",
      "                       '1) The trade-off between transfer and interference, '\n",
      "                       'which is one of the main contributions of this paper, '\n",
      "                       'has recently been pointed out by [1,2]. GEM[1] talks '\n",
      "                       'about it in terms of forward transfer and RWalk[2] in '\n",
      "                       'terms of \"intransigence\". Please clarify how '\n",
      "                       '\"transfer\" is different from these. A clear '\n",
      "                       'distinction will strengthen the contribution, '\n",
      "                       'otherwise, it seems like the paper talks about the '\n",
      "                       'same concepts with different terminologies, which will '\n",
      "                       'increase confusion in the literature.    \\n'\n",
      "                       '\\n'\n",
      "                       '2) Provide intuitions about equations (1) and (2). '\n",
      "                       'Also, why is this assumption correct in the case of '\n",
      "                       '\"incremental learning\" where the loss surface itself '\n",
      "                       'is changing for new tasks?\\n'\n",
      "                       '\\n'\n",
      "                       '3) The paper mentions that the performance for the '\n",
      "                       \"current task isn't an issue, which to me isn't that \"\n",
      "                       'obvious as if the evaluation setting is \"single-head '\n",
      "                       '[2]\" then the performance on current task becomes an '\n",
      "                       'issue as we move forwards over tasks because of the '\n",
      "                       'rigidity of the network to learn new tasks. Please '\n",
      "                       'clarify.\\n'\n",
      "                       '\\n'\n",
      "                       '4) In eq (4), the second sample (j) is also from the '\n",
      "                       'same dataset for which the loss is being minimized. '\n",
      "                       'Intuitively it makes sense to not to optimize loss for '\n",
      "                       'L(xj, yj) in order to enforce transfer. Please '\n",
      "                       'clarify.\\n'\n",
      "                       '\\n'\n",
      "                       '5) Since the claim is to improve the '\n",
      "                       '\"transfer-interference\" trade-off, how can we verify '\n",
      "                       'this just using accuracy? Any metric to quantify '\n",
      "                       'these? What about forgetting and forward transfer '\n",
      "                       'measures as discussed in [1,2]. Without these, its '\n",
      "                       'hard to say what exactly the algorithm is buying.\\n'\n",
      "                       '\\n'\n",
      "                       \"6) Why there isn't any result showing MER without \"\n",
      "                       'reservoir sampling. Also, please comment on the '\n",
      "                       'computational efficiency of the method (which is '\n",
      "                       'crucial for online learning), as it seems to be very '\n",
      "                       'slow. \\n'\n",
      "                       '\\n'\n",
      "                       '7)The supervised learning experiments are only shown '\n",
      "                       'on the MNIST. Maybe, at least show on CONV-NET/ RESNET '\n",
      "                       '(CIFAR etc).\\n'\n",
      "                       '\\n'\n",
      "                       '8) It is not clear from where the gains are coming. Do '\n",
      "                       'the ablation where instead of using two loops of '\n",
      "                       'reptile you use one loop.\\n'\n",
      "                       '\\n'\n",
      "                       'Minor:\\n'\n",
      "                       '=======\\n'\n",
      "                       '1) In the abstract, please clarify what you mean by '\n",
      "                       '\"future gradient\". Is it gradient over \"unseen\" task, '\n",
      "                       'or \"unseen\" data point of the same task. It\\'s clear '\n",
      "                       'after reading the manuscript, but takes a while to '\n",
      "                       'reach that stage.\\n'\n",
      "                       '2) Please clarify the difference between stationary '\n",
      "                       'and non-stationary distribution, or at least cite a '\n",
      "                       'paper with the proper definition.\\n'\n",
      "                       '3) Please define the problem precisely. Like a '\n",
      "                       'mathematical problem definition is missing which makes '\n",
      "                       'it hard to follow the paper. Clarify the evaluation '\n",
      "                       'setting (multi/single head etc [2])\\n'\n",
      "                       '4) No citation provided for \"reservoir sampling\" which '\n",
      "                       'is an important ingredient of this entire algorithm.\\n'\n",
      "                       '5) Please mention appendix sections as well when '\n",
      "                       'referred to appendix.\\n'\n",
      "                       '6) Provide citations for \"meta-learning\" in section '\n",
      "                       '1.\\n'\n",
      "                       '\\n'\n",
      "                       '\\n'\n",
      "                       '[1] GEM: Gradient episodic memory for continual '\n",
      "                       'learning, NIPS17.\\n'\n",
      "                       '[2] RWalk: Riemannian walk for incremental learning: '\n",
      "                       'Understanding forgetting and intransigence, ECCV2018.',\n",
      "             'title': 'Nice intuitions on how to think about transfer and '\n",
      "                      'interference, but not good enough technical '\n",
      "                      'contributions'},\n",
      " 'ddate': None,\n",
      " 'details': None,\n",
      " 'forum': 'B1gTShAct7',\n",
      " 'id': 'rkNyxxphX',\n",
      " 'invitation': 'ICLR.cc/2019/Conference/-/Paper1583/Official_Review',\n",
      " 'mdate': None,\n",
      " 'nonreaders': [],\n",
      " 'number': 3,\n",
      " 'original': None,\n",
      " 'readers': ['everyone'],\n",
      " 'referent': 'rye41gla2Q',\n",
      " 'replyto': 'B1gTShAct7',\n",
      " 'signatures': ['ICLR.cc/2019/Conference/Paper1583/AnonReviewer2'],\n",
      " 'tcdate': 1541369820403,\n",
      " 'tmdate': 1541533014706,\n",
      " 'writers': ['ICLR.cc/2019/Conference']}\n",
      "{'cdate': 1543845094385,\n",
      " 'content': {'confidence': '5: The reviewer is absolutely certain that the '\n",
      "                           'evaluation is correct and very familiar with the '\n",
      "                           'relevant literature',\n",
      "             'rating': '6: Marginally above acceptance threshold',\n",
      "             'review': 'The transfer/ interference perspective of lifelong '\n",
      "                       'learning is well motivated, and combining the '\n",
      "                       'meta-learning literature with the continual learning '\n",
      "                       'literature (applying reptile twice), even if seems '\n",
      "                       \"obvious, wasn't explored before. In addition, this \"\n",
      "                       'paper shows that a lot of gain can be obtained if one '\n",
      "                       'uses more randomized and representative memory '\n",
      "                       \"(reservoir sampling). However, I'm not entirely \"\n",
      "                       'convinced with the technical contributions and the '\n",
      "                       'analysis provided to support the claims in the paper, '\n",
      "                       'good enough for me to accept it in its current form. '\n",
      "                       \"Please find below my concerns and I'm more than happy \"\n",
      "                       'to change my mind if the answers are convincing.\\n'\n",
      "                       '\\n'\n",
      "                       'Main concerns:\\n'\n",
      "                       '\\n'\n",
      "                       '1) The trade-off between transfer and interference, '\n",
      "                       'which is one of the main contributions of this paper, '\n",
      "                       'has recently been pointed out by [1,2]. GEM[1] talks '\n",
      "                       'about it in terms of forward transfer and RWalk[2] in '\n",
      "                       'terms of \"intransigence\". Please clarify how '\n",
      "                       '\"transfer\" is different from these. A clear '\n",
      "                       'distinction will strengthen the contribution, '\n",
      "                       'otherwise, it seems like the paper talks about the '\n",
      "                       'same concepts with different terminologies, which will '\n",
      "                       'increase confusion in the literature.    \\n'\n",
      "                       '\\n'\n",
      "                       '2) Provide intuitions about equations (1) and (2). '\n",
      "                       'Also, why is this assumption correct in the case of '\n",
      "                       '\"incremental learning\" where the loss surface itself '\n",
      "                       'is changing for new tasks?\\n'\n",
      "                       '\\n'\n",
      "                       '3) The paper mentions that the performance for the '\n",
      "                       \"current task isn't an issue, which to me isn't that \"\n",
      "                       'obvious as if the evaluation setting is \"single-head '\n",
      "                       '[2]\" then the performance on current task becomes an '\n",
      "                       'issue as we move forwards over tasks because of the '\n",
      "                       'rigidity of the network to learn new tasks. Please '\n",
      "                       'clarify.\\n'\n",
      "                       '\\n'\n",
      "                       '4) In eq (4), the second sample (j) is also from the '\n",
      "                       'same dataset for which the loss is being minimized. '\n",
      "                       'Intuitively it makes sense to not to optimize loss for '\n",
      "                       'L(xj, yj) in order to enforce transfer. Please '\n",
      "                       'clarify.\\n'\n",
      "                       '\\n'\n",
      "                       '5) Since the claim is to improve the '\n",
      "                       '\"transfer-interference\" trade-off, how can we verify '\n",
      "                       'this just using accuracy? Any metric to quantify '\n",
      "                       'these? What about forgetting and forward transfer '\n",
      "                       'measures as discussed in [1,2]. Without these, its '\n",
      "                       'hard to say what exactly the algorithm is buying.\\n'\n",
      "                       '\\n'\n",
      "                       \"6) Why there isn't any result showing MER without \"\n",
      "                       'reservoir sampling. Also, please comment on the '\n",
      "                       'computational efficiency of the method (which is '\n",
      "                       'crucial for online learning), as it seems to be very '\n",
      "                       'slow. \\n'\n",
      "                       '\\n'\n",
      "                       '7)The supervised learning experiments are only shown '\n",
      "                       'on the MNIST. Maybe, at least show on CONV-NET/ RESNET '\n",
      "                       '(CIFAR etc).\\n'\n",
      "                       '\\n'\n",
      "                       '8) It is not clear from where the gains are coming. Do '\n",
      "                       'the ablation where instead of using two loops of '\n",
      "                       'reptile you use one loop.\\n'\n",
      "                       '\\n'\n",
      "                       'Minor:\\n'\n",
      "                       '=======\\n'\n",
      "                       '1) In the abstract, please clarify what you mean by '\n",
      "                       '\"future gradient\". Is it gradient over \"unseen\" task, '\n",
      "                       'or \"unseen\" data point of the same task. It\\'s clear '\n",
      "                       'after reading the manuscript, but takes a while to '\n",
      "                       'reach that stage.\\n'\n",
      "                       '2) Please clarify the difference between stationary '\n",
      "                       'and non-stationary distribution, or at least cite a '\n",
      "                       'paper with the proper definition.\\n'\n",
      "                       '3) Please define the problem precisely. Like a '\n",
      "                       'mathematical problem definition is missing which makes '\n",
      "                       'it hard to follow the paper. Clarify the evaluation '\n",
      "                       'setting (multi/single head etc [2])\\n'\n",
      "                       '4) No citation provided for \"reservoir sampling\" which '\n",
      "                       'is an important ingredient of this entire algorithm.\\n'\n",
      "                       '5) Please mention appendix sections as well when '\n",
      "                       'referred to appendix.\\n'\n",
      "                       '6) Provide citations for \"meta-learning\" in section '\n",
      "                       '1.\\n'\n",
      "                       '\\n'\n",
      "                       '\\n'\n",
      "                       '[1] GEM: Gradient episodic memory for continual '\n",
      "                       'learning, NIPS17.\\n'\n",
      "                       '[2] RWalk: Riemannian walk for incremental learning: '\n",
      "                       'Understanding forgetting and intransigence, ECCV2018.',\n",
      "             'title': 'Nice intuitions on how to think about transfer and '\n",
      "                      'interference, but not good enough technical '\n",
      "                      'contributions'},\n",
      " 'ddate': None,\n",
      " 'details': None,\n",
      " 'forum': 'B1gTShAct7',\n",
      " 'id': 'HkCyrnzkE',\n",
      " 'invitation': 'ICLR.cc/2019/Conference/-/Paper1583/Official_Review/AnonReviewer2/Revision',\n",
      " 'mdate': None,\n",
      " 'nonreaders': [],\n",
      " 'number': 1,\n",
      " 'original': None,\n",
      " 'readers': ['everyone'],\n",
      " 'referent': 'rye41gla2Q',\n",
      " 'replyto': 'B1gTShAct7',\n",
      " 'signatures': ['ICLR.cc/2019/Conference/Paper1583/AnonReviewer2'],\n",
      " 'tcdate': 1543845094385,\n",
      " 'tmdate': 1543845094385,\n",
      " 'writers': ['ICLR.cc/2019/Conference/Paper1583/AnonReviewer2',\n",
      "             'ICLR.cc/2019/Conference']}\n",
      "{'cdate': 1543846394830,\n",
      " 'content': {'confidence': '5: The reviewer is absolutely certain that the '\n",
      "                           'evaluation is correct and very familiar with the '\n",
      "                           'relevant literature',\n",
      "             'rating': '6: Marginally above acceptance threshold',\n",
      "             'review': 'The transfer/ interference perspective of lifelong '\n",
      "                       'learning is well motivated, and combining the '\n",
      "                       'meta-learning literature with the continual learning '\n",
      "                       'literature (applying reptile twice), even if seems '\n",
      "                       \"obvious, wasn't explored before. In addition, this \"\n",
      "                       'paper shows that a lot of gain can be obtained if one '\n",
      "                       'uses more randomized and representative memory '\n",
      "                       \"(reservoir sampling). However, I'm not entirely \"\n",
      "                       'convinced with the technical contributions and the '\n",
      "                       'analysis provided to support the claims in the paper, '\n",
      "                       'good enough for me to accept it in its current form. '\n",
      "                       \"Please find below my concerns and I'm more than happy \"\n",
      "                       'to change my mind if the answers are convincing.\\n'\n",
      "                       '\\n'\n",
      "                       'Main concerns:\\n'\n",
      "                       '\\n'\n",
      "                       '1) The trade-off between transfer and interference, '\n",
      "                       'which is one of the main contributions of this paper, '\n",
      "                       'has recently been pointed out by [1,2]. GEM[1] talks '\n",
      "                       'about it in terms of forward transfer and RWalk[2] in '\n",
      "                       'terms of \"intransigence\". Please clarify how '\n",
      "                       '\"transfer\" is different from these. A clear '\n",
      "                       'distinction will strengthen the contribution, '\n",
      "                       'otherwise, it seems like the paper talks about the '\n",
      "                       'same concepts with different terminologies, which will '\n",
      "                       'increase confusion in the literature.    \\n'\n",
      "                       '\\n'\n",
      "                       '2) Provide intuitions about equations (1) and (2). '\n",
      "                       'Also, why is this assumption correct in the case of '\n",
      "                       '\"incremental learning\" where the loss surface itself '\n",
      "                       'is changing for new tasks?\\n'\n",
      "                       '\\n'\n",
      "                       '3) The paper mentions that the performance for the '\n",
      "                       \"current task isn't an issue, which to me isn't that \"\n",
      "                       'obvious as if the evaluation setting is \"single-head '\n",
      "                       '[2]\" then the performance on current task becomes an '\n",
      "                       'issue as we move forwards over tasks because of the '\n",
      "                       'rigidity of the network to learn new tasks. Please '\n",
      "                       'clarify.\\n'\n",
      "                       '\\n'\n",
      "                       '4) In eq (4), the second sample (j) is also from the '\n",
      "                       'same dataset for which the loss is being minimized. '\n",
      "                       'Intuitively it makes sense to not to optimize loss for '\n",
      "                       'L(xj, yj) in order to enforce transfer. Please '\n",
      "                       'clarify.\\n'\n",
      "                       '\\n'\n",
      "                       '5) Since the claim is to improve the '\n",
      "                       '\"transfer-interference\" trade-off, how can we verify '\n",
      "                       'this just using accuracy? Any metric to quantify '\n",
      "                       'these? What about forgetting and forward transfer '\n",
      "                       'measures as discussed in [1,2]. Without these, its '\n",
      "                       'hard to say what exactly the algorithm is buying.\\n'\n",
      "                       '\\n'\n",
      "                       \"6) Why there isn't any result showing MER without \"\n",
      "                       'reservoir sampling. Also, please comment on the '\n",
      "                       'computational efficiency of the method (which is '\n",
      "                       'crucial for online learning), as it seems to be very '\n",
      "                       'slow. \\n'\n",
      "                       '\\n'\n",
      "                       '7)The supervised learning experiments are only shown '\n",
      "                       'on the MNIST. Maybe, at least show on CONV-NET/ RESNET '\n",
      "                       '(CIFAR etc).\\n'\n",
      "                       '\\n'\n",
      "                       '8) It is not clear from where the gains are coming. Do '\n",
      "                       'the ablation where instead of using two loops of '\n",
      "                       'reptile you use one loop.\\n'\n",
      "                       '\\n'\n",
      "                       'Minor:\\n'\n",
      "                       '=======\\n'\n",
      "                       '1) In the abstract, please clarify what you mean by '\n",
      "                       '\"future gradient\". Is it gradient over \"unseen\" task, '\n",
      "                       'or \"unseen\" data point of the same task. It\\'s clear '\n",
      "                       'after reading the manuscript, but takes a while to '\n",
      "                       'reach that stage.\\n'\n",
      "                       '2) Please clarify the difference between stationary '\n",
      "                       'and non-stationary distribution, or at least cite a '\n",
      "                       'paper with the proper definition.\\n'\n",
      "                       '3) Please define the problem precisely. Like a '\n",
      "                       'mathematical problem definition is missing which makes '\n",
      "                       'it hard to follow the paper. Clarify the evaluation '\n",
      "                       'setting (multi/single head etc [2])\\n'\n",
      "                       '4) No citation provided for \"reservoir sampling\" which '\n",
      "                       'is an important ingredient of this entire algorithm.\\n'\n",
      "                       '5) Please mention appendix sections as well when '\n",
      "                       'referred to appendix.\\n'\n",
      "                       '6) Provide citations for \"meta-learning\" in section '\n",
      "                       '1.\\n'\n",
      "                       '\\n'\n",
      "                       '\\n'\n",
      "                       '[1] GEM: Gradient episodic memory for continual '\n",
      "                       'learning, NIPS17.\\n'\n",
      "                       '[2] RWalk: Riemannian walk for incremental learning: '\n",
      "                       'Understanding forgetting and intransigence, ECCV2018.',\n",
      "             'title': 'Nice intuitions on how to think about transfer and '\n",
      "                      'interference (thorough rebuttal convinced me to upgrade '\n",
      "                      'my rating)'},\n",
      " 'ddate': None,\n",
      " 'details': None,\n",
      " 'forum': 'B1gTShAct7',\n",
      " 'id': 'H1XZc3z14',\n",
      " 'invitation': 'ICLR.cc/2019/Conference/-/Paper1583/Official_Review/AnonReviewer2/Revision',\n",
      " 'mdate': None,\n",
      " 'nonreaders': [],\n",
      " 'number': 2,\n",
      " 'original': None,\n",
      " 'readers': ['everyone'],\n",
      " 'referent': 'rye41gla2Q',\n",
      " 'replyto': 'B1gTShAct7',\n",
      " 'signatures': ['ICLR.cc/2019/Conference/Paper1583/AnonReviewer2'],\n",
      " 'tcdate': 1543846394830,\n",
      " 'tmdate': 1543846394830,\n",
      " 'writers': ['ICLR.cc/2019/Conference/Paper1583/AnonReviewer2',\n",
      "             'ICLR.cc/2019/Conference']}\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 10/10 [01:14<00:00,  7.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "import json\n",
    "import openreview\n",
    "import os\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "INVITATION = 'ICLR.cc/2019/Conference/-/Blind_Submission'\n",
    "\n",
    "invitation_map = {\"ICLR_2019\": 'ICLR.cc/2019/Conference/-/Blind_Submission'}\n",
    "\n",
    "LIMIT = 10  # Number of papers to build timelines for\n",
    "pdfs_dir = \"another_dir/\"\n",
    "\n",
    "# A client is required for any OpenReview API actions\n",
    "guest_client = openreview.Client(baseurl='https://api.openreview.net')\n",
    "\n",
    "# ==== HELPERS\n",
    "\n",
    "def maybe_transform_date(timestamp):\n",
    "  if timestamp is None:\n",
    "    return None\n",
    "  else:\n",
    "    return datetime.fromtimestamp(int(timestamp/1000)).strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
    "\n",
    "\n",
    "def get_initiator(note):\n",
    "    return note.signatures[0].split(\"/\")[-1]\n",
    "    \n",
    "class PDFStatus(object):\n",
    "  AVAILABLE = \"available\"\n",
    "  DUPLICATE = \"duplicate\"\n",
    "  FORBIDDEN = \"forbidden\"\n",
    "  NOT_FOUND = \"not_found\"\n",
    "  NOT_APPLICABLE = \"not_applicable\"\n",
    "  \n",
    "  \n",
    "class EventType(object):\n",
    "  COMMENT = \"comment\"\n",
    "  ARTIFACT = \"artifact\"\n",
    "\n",
    "def make_metadata_dict(reference, file_path, reference_index, pdf_status, event_type):\n",
    "  return {\n",
    "    \"forum\": reference.forum,\n",
    "    \"initiator\": get_initiator(reference),\n",
    "    \"identifier\": reference.id,\n",
    "    \"tcdate\": reference.tcdate,\n",
    "    \"tmdate\": reference.tmdate,\n",
    "    \"reply_to\": reference.replyto,\n",
    "    \"reference_index\": reference_index,\n",
    "    \"pdf_status\": pdf_status,\n",
    "    \"event_type\": event_type,\n",
    "    \"filepath\": file_path,\n",
    "  }\n",
    "\n",
    "ERROR_STATUS_LOOKUP = {\n",
    "  \"ForbiddenError\": PDFStatus.FORBIDDEN,\n",
    "  \"NotFoundError\": PDFStatus.NOT_FOUND,\n",
    "}\n",
    "      \n",
    "def write_artifact(conference, note_id, reference, reference_index, directories, checksum_map):\n",
    "  print(reference.original, note_id, reference.id)\n",
    "  pdf_path = make_path(directories, f'artifact_{note_id}_{reference_index}.pdf')\n",
    "  is_reference = not reference.id == reference.forum\n",
    "  try: # try to get the PDF for this reference\n",
    "    pdf_binary = guest_client.get_pdf(reference.id, is_reference=is_reference)\n",
    "    this_checksum = hashlib.md5(pdf_binary).hexdigest()\n",
    "    found = False\n",
    "    for other_pdf_path, other_checksum in checksum_map.items():\n",
    "      if other_checksum == this_checksum:\n",
    "        pdf_path = other_pdf_path\n",
    "        pdf_status = PDFStatus.DUPLICATE\n",
    "        found = True\n",
    "    if not found:\n",
    "      checksum_map[pdf_path] = this_checksum\n",
    "      with open(pdf_path, 'wb') as file_handle:\n",
    "        file_handle.write(pdf_binary)\n",
    "      pdf_status = PDFStatus.AVAILABLE\n",
    "  except openreview.OpenReviewException as e:\n",
    "    error_name = e.args[0]['name']\n",
    "    pdf_status = ERROR_STATUS_LOOKUP[error_name]\n",
    "      \n",
    "  return checksum_map, make_metadata_dict(reference, pdf_path, reference_index, pdf_status, EventType.ARTIFACT)\n",
    "\n",
    "\n",
    "def make_path(directories, filename=None):\n",
    "  directory = os.path.join(*directories)\n",
    "  os.makedirs(directory, exist_ok=True)\n",
    "  if filename is not None:\n",
    "    return os.path.join(*directories, filename)\n",
    "    \n",
    "\n",
    "\n",
    "def write_comment(conference, note_id, reference, reference_index, directories):\n",
    "  json_path = make_path(directories, f'comment_{note_id}_{reference_index}.json')\n",
    "  is_reference = not reference.id == reference.forum\n",
    "  with open(json_path, 'w') as f:\n",
    "    json.dump(reference.content, f)\n",
    "  return make_metadata_dict(reference, json_path, reference_index, PDFStatus.NOT_APPLICABLE, EventType.COMMENT)\n",
    "  \n",
    "def get_references_in_order(references):\n",
    "  return list(sorted(references, key=lambda x: x.tmdate))\n",
    "\n",
    "\n",
    "events = []\n",
    "\n",
    "forum_to_events_map = collections.defaultdict(list)\n",
    "\n",
    "for conference, invitation in invitation_map.items():\n",
    "  forum_notes = list(openreview.tools.iterget_notes(guest_client, invitation=invitation))[:LIMIT]\n",
    "  for forum_note in tqdm.tqdm(forum_notes):\n",
    "    dir_path = [pdfs_dir, conference, forum_note.id]\n",
    "    make_path(dir_path)\n",
    "    # Get PDFs from main submission/ BlindSubmission (?)\n",
    "    checksum_map = {}\n",
    "    references = get_references_in_order(guest_client.get_references(referent=forum_note.id, original=True))\n",
    "    for ref_i, reference in enumerate(references): # We don't run this with original=False. This would give us the revisions to the Blind Submission itself, rather than the original submission note.\n",
    "      checksum_map, row = write_artifact(conference, forum_note.id, reference, ref_i, dir_path, checksum_map)\n",
    "      events.append(row)\n",
    "    \n",
    "    # Get PDFs from comments\n",
    "    for note in guest_client.get_notes(forum=forum_note.id):\n",
    "      if note.id == forum_note.id: # Already done above\n",
    "        \n",
    "        print(len(list(guest_client.get_references(referent=note.id, original=False))))\n",
    "      references = get_references_in_order(guest_client.get_references(referent=note.id, original=False))\n",
    "      if len(references) == 1:\n",
    "        continue\n",
    "      for ref_i, reference in enumerate(references):\n",
    "        print(reference)\n",
    "        \n",
    "        events.append(write_comment(conference, note.id, reference, ref_i, dir_path))\n",
    "      print(\"+\" * 80)\n",
    "df = pd.DataFrame.from_dict(events)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a441b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a501ac3a",
   "metadata": {},
   "source": [
    "* My current understanding is that using `client.get_references` should get me all revisions of a note.\n",
    "  * If it's the top note of a forum (i.e. `note.id == note.forum`, then it contains revisions of the submission pdf.\n",
    "  * If it's any other note, then it contains revisions of the comment\n",
    "* I hoped to get some information about the meanings of these by looking at them on the OpenReview website using permalinks, but as I understand it the individual referent_ids are not visible on the website. However, I can go to https://openreview.net/revisions?id={forum_id} or https://openreview.net/revisions?id={note_id} and see them listed.\n",
    "\n",
    "# Questions\n",
    "\n",
    "* I can't figure out what the `original` flag does. It's supposed to \"additionally return references to the original note\" per [this file](https://github.com/openreview/openreview-py/blob/db643b015e0f46aec66c2b1227fe0cd2e21b621d/openreview/openreview.py#L1091).\n",
    "  * However, sometimes it does not result in any change in the result (e.g. line 10 and 11 in the table, almost the same function call with `original` set to either true or false results in the same result both times)\n",
    "  * In other cases, setting `original` to true or false results in completely disjoint sets of things returned, e.g. lines 22-29 and line 30)\n",
    "* I'm also trying to understand how the different date fields pertain to the modifications.\n",
    "  * It seems that `mdate` is always None, is that expected?\n",
    "  * For something with revisions, e.g. r1ghyrDc2m in lines 14-17 ([link to revisions](https://openreview.net/revisions?id=r1ghyrDc2m)), I would expect the modification date to be available in tmdate, perhaps, so something like November 6th. However tcdate, tmdate, and cdate all have the same value for all revisions. Where would the November 6th date referred to on the revisions page be stored?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd912d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import tqdm\n",
    "# pdfs_dir = \"another_dir/\"\n",
    "# for i, forum_note in enumerate(openreview.tools.iterget_notes(\n",
    "#         guest_client, invitation=INVITATION)):\n",
    "#     write_pdf_to_file(guest_client, forum_dir, forum_note)\n",
    "#     for revision in tqdm.tqdm(guest_client.get_references(\n",
    "#             referent=forum_note.id, original=True),\n",
    "#             desc=\"Getting revisions for {0}\".format(forum_note.id)):\n",
    "#         try:\n",
    "#             write_pdf_to_file(guest_client, forum_dir, revision)\n",
    "#         except openreview.OpenReviewException as e:\n",
    "#             print(e)\n",
    "#             continue\n",
    "#     if i == LIMIT + 1:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbc7cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
